[
  {
    "objectID": "CITATION.html",
    "href": "CITATION.html",
    "title": "Citation",
    "section": "",
    "text": "Citation\nTo cite package ‘ordbetareg’ in publications use:\n\n  Kubinec R (????). _ordbetareg: Ordered Beta Regression Models with 'brms'_. R package\n  version 0.7.3.2.\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {ordbetareg: Ordered Beta Regression Models with 'brms'},\n    author = {Robert Kubinec},\n    note = {R package version 0.7.3.2},\n  }",
    "crumbs": [
      "Citation"
    ]
  },
  {
    "objectID": "NEWS.html",
    "href": "NEWS.html",
    "title": "ordbetareg v0.7.3.1",
    "section": "",
    "text": "ordbetareg v0.7.3.1\n\nremoved transformr from imports because of many dependencies\nfixed bug if the intercept was specified and centered due to brms code change\nfixed bug that prevented the make_stancode option from working. changed that option to return_stancode\nfixed pp_check_ordbeta for multivariate DVs\n\n\n\nordbetareg v0.7.2\n\nglmmTMB no longer has standard errors from the marginaleffects package. Updated vignette to reflect this.\n\n\n\nordbetareg v0.7.1\n\nRemoved dependency on faux as faux was taken off CRAN. This also means the sim_ordbeta function cannot generate correlated covariates.\nFixed a bug with the transform option to the slopes function in marginaleffects in the vignette.\nmarginaleffects now supports standard errors for ordered beta regression models in glmmTMB, so vignette was updated to reflect this.\n\n\n\nordbetareg v0.7.0\n\nAllow intercepts to receive separate priors from main effects (permitting zero-ing out intercepts).\nChanged log-likelihood calculation to match brms documentation and permit point-wise loo calculation.\nAdded animated density plots for continuous responses.\nImproved plot formatting and added theme and label options.\nUpdated vignette to include information about glmmTMB as an alternative for estimation and new plot functions.\nAdded distribution rordbeta and dordbeta functions.\nRemoved legacy 0 + Intercept function code. Now accepts any kind of formula.\n\n\n\nordbetareg v0.5.0\n\nFixed bug disabling extra priors for phi regression/modeling.\nAdded pp_check_ordbetareg function for accurate posterior predictive plots for discrete and continuous outcomes.\nUpdated vignette and documentation.\n\n\n\nordbetareg v0.3.0\n\nFixed bugs relating to processing of bounded outcomes.\nAllow for user-specific bounds to be set for normalization of outcome to 0/1 (issue #2).\nEnable multiple imputation and multivariate responses with brms.\nFix documentation issues (issues #4 and #5).\n\n\n\nordbetareg v0.2.1\n\nAdded RNG code for the induced_dirichlet prior on cutpoints to allow for the sample_prior option to work with brm, allowing for Bayes factors and other post-estimation tools with the bridgesampling package (issue #1).\nFixed error in terms of correctly accounting for reverse transform of the ordered cutpoint vector.\nMade the ldpf function more stable by using logs instead of direct multiplication.\nFixed typos in the vignette.\n\n\n\nordbetareg v0.2\n\nInitial release.",
    "crumbs": [
      "News"
    ]
  },
  {
    "objectID": "vignettes/package_introduction.html",
    "href": "vignettes/package_introduction.html",
    "title": "Introduction to ordbetareg",
    "section": "",
    "text": "The ordered beta regression model is designed explicitly for data with upper and lower bounds, such as survey slider scales, dose/response relationships, and anything that can be considered a proportion or a percentage. This type of data cannot be fit with the standard beta regression model because the beta distribution does not allow for any observations at the bounds, such as a percentage/proportion of 0% or 100%. The ordered beta regression model solves this problem by combining the beta distribution with an ordinal distribution over continuous and discrete, or degenerate, observations at the bounds. It is an efficient model that produces intelligible estimates that also respect the bounds of the dependent variable.\nPlease note that this is not the only implementation of ordered beta regression. For more info on alternatives, including other statistical software packages like Python or Julia, see the package website.\nThis vignette covers a number of topics, from data preparation (Data Preparation), fitting a model ([Run In ordbetareg]), visualizing results (Visualization), interpreting results (How to Report Results), using multiple imputed datasets (Multivariate and Missing Data), performing mediation analysis (Multivariate Modeling and Mediation Analysis), modeling ancillary parameters like dispersion and cutpoints (Modeling Dispersion and Cutpoints), and power analysis for experimental designs (Power Analysis).\nFor more information about the ordered beta statistical distribution, I refer you to my paper on the model for more details (also there is an ungated version available here).\n\n\nIf you’ve enjoyed using the package, consider buying some ordbetareg swag! Click the image below to go to the online store (I receive a few dollars from each order).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis notebook contains instructions for running the ordered beta regression model in the R package ordbetareg. ordbetareg is a front-end to brms, a very powerful regression modeling package based on the Stan Hamiltonian Markov Chain Monte Carlo sampler. I only show in this vignette a small part of the features which are available via brms, and I refer the user to the copious documentation describing the features of the brms package. Suffice it to say that most kinds of regression models can be fit with the software, including hierarchical, dynamic, nonlinear and multivariate models (or all of the above in combination). The ordbetareg package allows for all of these features to be used with the ordered regression model distribution by adding this distribution to brms.\n\n\n\nThe reproducible Quarto file this vignette is compiled from can be accessed from the package Github site.\nIf you use the model in a paper, please cite it as:\nKubinec, Robert. 2023. “Ordered Beta Regression: A Parsimonious, Well-Fitting Model for Continuous Data with Lower and Upper Bounds.” Political Analysis 31(4): 519–36. doi: 10.1017/pan.2022.20.\n\n\n# whether to run models from scratch\nrun_model &lt;- F",
    "crumbs": [
      "Articles",
      "Introduction to `ordbetareg`"
    ]
  },
  {
    "objectID": "vignettes/package_introduction.html#support-ordbetareg",
    "href": "vignettes/package_introduction.html#support-ordbetareg",
    "title": "Introduction to ordbetareg",
    "section": "",
    "text": "If you’ve enjoyed using the package, consider buying some ordbetareg swag! Click the image below to go to the online store (I receive a few dollars from each order).",
    "crumbs": [
      "Articles",
      "Introduction to `ordbetareg`"
    ]
  },
  {
    "objectID": "vignettes/package_introduction.html#ordbetareg-and-brms",
    "href": "vignettes/package_introduction.html#ordbetareg-and-brms",
    "title": "Introduction to ordbetareg",
    "section": "",
    "text": "This notebook contains instructions for running the ordered beta regression model in the R package ordbetareg. ordbetareg is a front-end to brms, a very powerful regression modeling package based on the Stan Hamiltonian Markov Chain Monte Carlo sampler. I only show in this vignette a small part of the features which are available via brms, and I refer the user to the copious documentation describing the features of the brms package. Suffice it to say that most kinds of regression models can be fit with the software, including hierarchical, dynamic, nonlinear and multivariate models (or all of the above in combination). The ordbetareg package allows for all of these features to be used with the ordered regression model distribution by adding this distribution to brms.",
    "crumbs": [
      "Articles",
      "Introduction to `ordbetareg`"
    ]
  },
  {
    "objectID": "vignettes/package_introduction.html#citation",
    "href": "vignettes/package_introduction.html#citation",
    "title": "Introduction to ordbetareg",
    "section": "",
    "text": "The reproducible Quarto file this vignette is compiled from can be accessed from the package Github site.\nIf you use the model in a paper, please cite it as:\nKubinec, Robert. 2023. “Ordered Beta Regression: A Parsimonious, Well-Fitting Model for Continuous Data with Lower and Upper Bounds.” Political Analysis 31(4): 519–36. doi: 10.1017/pan.2022.20.\n\n\n# whether to run models from scratch\nrun_model &lt;- F",
    "crumbs": [
      "Articles",
      "Introduction to `ordbetareg`"
    ]
  },
  {
    "objectID": "vignettes/package_introduction.html#multivariate-and-missing-data",
    "href": "vignettes/package_introduction.html#multivariate-and-missing-data",
    "title": "Introduction to ordbetareg",
    "section": "Multivariate and Missing Data",
    "text": "Multivariate and Missing Data\nIf you want to use some of brms more powerful techniques, such as multivariate modeling, you can also pass the result of a bf function call to the formula argument. Doing so allows you to use mixed distributions, such as modeling one response as ordered beta and the other as normal/Gaussian. I refer you to the brmsformula() function help for more details.\nYou can also make use of brms missing data imputation abilities if you pass a list of imputed datasets as a list and set the use_brms_multiple to TRUE. For more details on how to prepare data for brms multiple imputation, see https://cran.r-project.org/web/packages/brms/vignettes/brms_missings.html.",
    "crumbs": [
      "Articles",
      "Introduction to `ordbetareg`"
    ]
  },
  {
    "objectID": "vignettes/package_introduction.html#example-feeling-thermometer-towards-professors",
    "href": "vignettes/package_introduction.html#example-feeling-thermometer-towards-professors",
    "title": "Introduction to ordbetareg",
    "section": "Example: Feeling Thermometer Towards Professors",
    "text": "Example: Feeling Thermometer Towards Professors\nTo demonstrate some of the power of using brms as a regression engine, I will model education and income as ordinal predictors by using the mo() function in the formula definition. By doing so, we can get a single effect for education and income instead of having to use dummies for separate education/income categories. As a result, I can include an interaction between the two variables to see if wealthier more educated people have better views towards college professors than poorer better educated people. Finally, I include varying (random) census region intercepts.\n\n\nif(run_model) {\n  \n  ord_fit_mean &lt;- ordbetareg(formula=therm ~ mo(education)*mo(income) +\n                               (1|region), \n                       data=model_data,\n                       control=list(adapt_delta=0.95),\n                cores=1,chains=1,iter=500,\n                refresh=0)\n                # NOTE: to do parallel processing within chains\n                # add the options below\n                #threads=5,\n                #backend=\"cmdstanr\"\n                #where threads is the number of cores per chain\n                # you must have cmdstanr set up to do so\n                # see https://mc-stan.org/cmdstanr/\n  \n} else {\n  \n  data(\"ord_fit_mean\")\n  \n}\n\nBecause this model is somewhat complicated with multiple ordinal factors and multilevel intercepts, I use the option adapt_delta=0.95 to improve sampling and remove divergent transitions.",
    "crumbs": [
      "Articles",
      "Introduction to `ordbetareg`"
    ]
  },
  {
    "objectID": "vignettes/package_introduction.html#priors",
    "href": "vignettes/package_introduction.html#priors",
    "title": "Introduction to ordbetareg",
    "section": "Priors",
    "text": "Priors\nThe default priors in ordbetareg are weakly informative for data that is scaled roughly between \\([-10,10]\\). Outside of those scales, you may want to increase the SD of the Normal prior on regression coefficients via the coef_prior_sd and/or phi_coef_prior_sd options to ordbetareg (depending on whether the prior is for the coefficients in the main outcome model or the auxiliary dispersion phi regression). If you need to set a specific prior on the intercept, pass values to the intercept_prior_mean/phi_intercept_prior_mean and intercept_prior_sd/phi_intercept_prior_sd parameters (you need to have values for both for the prior to be set correctly) to set a Normally-distributed prior on the intercept. You can also zero out the intercept by setting a tight prior around 0, such as intercept_prior_mean=0 and intercept_prior_sd=0.001.\nIf you need to do something more involved with priors, you can use the extra_prior option. This argument can take any brms-specified prior, including any of the many brms options for priors. If you want to set your own priors and ignore all the prior-related arguments, you can also use the manual_prior option and again pass a brms-compliant prior object. This is necessary if you want to change the cutpoints or model the cutpoints directly, as I explain below.\nSetting manual/more specific priors are beyond the scope of the vignette, but I refer the reader to the brms documentation for more info.",
    "crumbs": [
      "Articles",
      "Introduction to `ordbetareg`"
    ]
  },
  {
    "objectID": "vignettes/package_introduction.html#visualization",
    "href": "vignettes/package_introduction.html#visualization",
    "title": "Introduction to ordbetareg",
    "section": "Visualization",
    "text": "Visualization\nWe can visualize the model cutpoints by showing them relative to the empirical distribution. To do so, we have to transform the cutpoints using the inverse logit function in R (plogis) to get back values in the scale of the response, and I have to exponentiate and add the first cutpoint to get the correct value for the second cutpoint. The plot shows essentially how spread-out the cutpoints are relative to the data, though it should be noted that the analogy is inexact–it is not the case that observations above or below the cutpoints are considered to be discrete or continuous. Rather, as distance from the cutpoints increases, the probability of a discrete or continuous response increases. Cutpoints that are far apart indicate a data distribution that shows substantial differences between continuous and discrete observations.\n\n\nall_draws &lt;- prepare_predictions(ord_fit_mean)\n\ncutzero &lt;- plogis(all_draws$dpars$cutzero)\ncutone &lt;- plogis(all_draws$dpars$cutzero + exp(all_draws$dpars$cutone))\n\npew %&gt;% \n  ggplot(aes(x=therm)) +\n  geom_histogram(bins=100) +\n  theme_minimal() + \n  theme(panel.grid=element_blank()) +\n  scale_x_continuous(breaks=c(0,25,50,75,100),\n                     labels=c(\"0\",\"Colder\",\"50\",\"Warmer\",\"100\")) +\n  geom_vline(xintercept = mean(cutzero)*100,linetype=2) +\n  geom_vline(xintercept = mean(cutone)*100,linetype=2) +\n  ylab(\"\") +\n  xlab(\"\") +\n  labs(caption=paste0(\"Figure shows the distribution of \",sum(!is.na(pew$therm)),\" non-missing survey responses.\"))\n\n\n\n\n\n\n\n\nThe plot shows that the model sees significant heterogeneity between the discrete responses at the bounds and the continuous responses in the plot.",
    "crumbs": [
      "Articles",
      "Introduction to `ordbetareg`"
    ]
  },
  {
    "objectID": "vignettes/package_introduction.html#posterior-predictive-plot",
    "href": "vignettes/package_introduction.html#posterior-predictive-plot",
    "title": "Introduction to ordbetareg",
    "section": "Posterior Predictive Plot",
    "text": "Posterior Predictive Plot\nThe best way to visualize model fit is to plot the full predictive distribution relative to the original outcome. Because ordered beta regression is a mixed discrete/continuous model, a separate plotting function, pp_check_ordbeta, is included that accurately handles the unique features of this distribution. This function returns a list with two plots, discrete and continuous, which can either be printed and plotted or further modified as ggplot2 objects.\nThe discrete plot, which is a bar graph, shows that the posterior distribution accurately captures the number of different types of responses (discrete or continuous) in the data. For the continuous plot, shown as a density plot with one line per posterior draw, the model can’t capture all of the modality in the distribution – there are effectively four separate modes – but it is reasonably accurate over the middle responses and the responses near the bounds.\n\n\n# new theme option will add in new ggplot2 themes or themes\n# from other packages\n\nplots &lt;- pp_check_ordbeta(ord_fit_mean,\n                          ndraws=100,\n                          outcome_label=\"Thermometer Rating\",\n                          new_theme=ggthemes::theme_economist())\n\nplots$discrete\n\n\n\n\n\n\n\nplots$continuous\n\n\n\n\n\n\n\n\nThe pp_check_ordbeta function can also show each posterior draw one at a time in an animated plot if the animate option is set to TRUE. This option requires the installation of the additional packages gganimate and transformr.",
    "crumbs": [
      "Articles",
      "Introduction to `ordbetareg`"
    ]
  },
  {
    "objectID": "vignettes/package_introduction.html#heiss-plot",
    "href": "vignettes/package_introduction.html#heiss-plot",
    "title": "Introduction to ordbetareg",
    "section": "Heiss Plot",
    "text": "Heiss Plot\nordbetareg also includes a plot derived by the statistician Andrew Heiss. This plot visualizes the three components of the outcome–the lower bound, the middle continuous part, and the upper bound–as a function of a factor/discrete variable. This plot is helpful at understanding how discrete variables might be affecting the responses at the bounds (i.e., extreme responses). Importantly, this plot shows uncertainty by plotting these predictions for a set number of posterior draws (be careful about using too many draws as it can slow down the plot quite a bit).\nBelow is a Heiss plot for our example regression model by showing the predicted components of the outcome for each value of education:\n\n\n# turn off caption as it doesn't fit well in vignette mode\n\nplot_heiss(ord_fit_mean, grouping_fac=\"education\", ndraws = 100,\n           plot_caption = \"\")\n\n\n\n\n\n\n\n\nAs can be seen, the plot allows for intuitive comparisons of the predicted values at the bounds of the scale across levels of a discrete predictor.",
    "crumbs": [
      "Articles",
      "Introduction to `ordbetareg`"
    ]
  },
  {
    "objectID": "vignettes/package_introduction.html#how-to-report-results",
    "href": "vignettes/package_introduction.html#how-to-report-results",
    "title": "Introduction to ordbetareg",
    "section": "How to Report Results",
    "text": "How to Report Results\nFor many analyses, coefficient tables are quite valuable for reporting model results. I first show how to report the raw coefficients, although it is important to note that these coefficients are not all that interesting as they are on the logit scale (and no, you can’t interpret them as log-odds or what have you). It is more interesting to report the [marginal effects], as I define below, and leave the raw coefficients in an appendix.\nWe can see the raw coefficients from the model in table form using the modelsummary package, which has support for brms models. We’ll specify only confidence intervals as other frequentist statistics have no Bayesian analogue (i.e. p-values). We’ll also specify only the main effects of the ordinal predictors, and give them more informative names.\n\n\nlibrary(modelsummary)\n\nmodelsummary(ord_fit_mean,statistic = \"conf.int\",\n             metrics = \"RMSE\",\n             coef_map=c(\"b_Intercept\"=\"Intercept\",\n                        \"bsp_moeducation\"=\"Education\",\n                        \"bsp_moincome\"=\"Income\",\n                        \"bsp_moeducation:moincome\"=\"EducationXIncome\"))\n\n\n\n\n\n\n\n\n(1)\n\n\n\n\nIntercept\n0.183\n\n\n\n[-0.013, 0.388]\n\n\nEducation\n0.115\n\n\n\n[0.014, 0.188]\n\n\nIncome\n-0.052\n\n\n\n[-0.090, -0.013]\n\n\nEducationXIncome\n0.007\n\n\n\n[-0.008, 0.023]\n\n\nNum.Obs.\n2431\n\n\nRMSE\n0.29\n\n\n\n\n\n\n\nNote that this plot does not show the cut point coefficients or phi due to the use of the coef_map function. Removing that argument would output all coefficients. modelsummary tables have many more options, including output to both html and latex formats. I refer the reader to the package documentation for more info.\nAs I mentioned earlier, you can interpret sign and significance from the raw coefficients, but substantively, you need marginal effects (see below).\n\nMarginal Effects\nThere is a related package, marginaleffects, that allows us to convert these coefficients into more meaningful marginal effect estimates, i.e., the effect of the predictors expresses as the actual change in the outcome on the 0 - 1 scale. If those aren’t the original bounds of your scale, you can convert the effects to any bounded scale as well (such as -5 and 78 or 1 and 100).\nBecause we have two variables in our model that are both ordinal in nature, marginaleffects will produce an estimate of the marginal effect of each value of each ordinal predictor. I use the avg_slopes function to convert the ordbetareg model to a data frame of marginal/conditional effects in the scale of the outcome that can be easily printed:\n\n\navg_slopes(ord_fit_mean, variables=\"education\") %&gt;%\n  select(Variable=\"term\",\n         Level=\"contrast\",\n         `5% Quantile`=\"conf.low\",\n         `Posterior Mean`=\"estimate\",\n         `95% Quantile`=\"conf.high\") %&gt;% \n  knitr::kable(caption = \"Marginal Effect of Education on Professor Thermometer\",\n               format.args=list(digits=2),\n               align=c('llccc'))\n\n\nMarginal Effect of Education on Professor Thermometer\n\n\n\n\n\n\n\n\n\nVariable\nLevel\n5% Quantile\nPosterior Mean\n95% Quantile\n\n\n\n\neducation\nAssociate’s degree - Less than high school\n0.0157\n0.070\n0.129\n\n\neducation\nCollege graduate/some postgrad - Less than high school\n0.0795\n0.130\n0.177\n\n\neducation\nHigh school graduate - Less than high school\n-0.0048\n0.015\n0.055\n\n\neducation\nPostgraduate - Less than high school\n0.1500\n0.201\n0.245\n\n\neducation\nSome college, no degree - Less than high school\n0.0059\n0.048\n0.101\n\n\n\n\n\nWe can see that we have separate marginal effects for each level of education due to modeling it as an ordinal predictor. At present the avg_slopes() function cannot calculate a single effect, though that is possible manually. As we can see with the raw coefficient in the table above, the marginal effects are all positive, though the magnitude varies across different levels of education.\nNote as well that the slopes function can calculate unit-level (row-wise) estimates of the predictors on the outcome (see marginaleffects documentation for more info).",
    "crumbs": [
      "Articles",
      "Introduction to `ordbetareg`"
    ]
  },
  {
    "objectID": "vignettes/package_introduction.html#modeling-cutpoints",
    "href": "vignettes/package_introduction.html#modeling-cutpoints",
    "title": "Introduction to ordbetareg",
    "section": "Modeling Cutpoints",
    "text": "Modeling Cutpoints\nYou can also use brms distributional regression to have covariates that predict the location of the cutpoints. This type of regression would be useful if you want to see if there is variation in the heterogeneity of the scale (i.e., how extreme the bounds are) across levels of a factor. While this is likely only relevant for specific research questions, I include example code below for how to fit this model in case it is of interest. Note that the manual_prior option must be used to set priors on regression coefficients as otherwise the model will not have any priors on these coefficients and will not fit well.\n\n# we'll use our data, although we won't fit this model\n\ncutpoint_mod &lt;- ordbetareg(formula=bf(therm ~ education +\n                               (1|region),\n                               cutzero ~ education,\n                               cutone ~ education),\n                       data=model_data,\n                       manual_prior=set_prior(\"normal(0,5)\") +\n                    set_prior(\"normal(0,5)\", class=\"b\",dpar=\"cutone\") +\n                    set_prior(\"normal(0,5)\", class=\"b\",dpar=\"cutzero\"),\n                       control=list(adapt_delta=0.95),\n                cores=1,chains=1,iter=500,\n                refresh=0)\n\nIn the brms code above, cutzero is the bottom cutpoint and cutone is the top cut point, and they each receive their own formula in addition to the main formula for the response. These three formulas can be combined with the bf function as shown above. I also set loosely informative Normal priors on the regression coefficients for each of these three sub-models (note you must set explicit priors for both cutzero and cutone). These predictors will then appear in the summary function with cutzero or cutone appended to the name of the predictor (i.e. education).",
    "crumbs": [
      "Articles",
      "Introduction to `ordbetareg`"
    ]
  },
  {
    "objectID": "vignettes/package_introduction.html#sim_ordbeta-function",
    "href": "vignettes/package_introduction.html#sim_ordbeta-function",
    "title": "Introduction to ordbetareg",
    "section": "sim_ordbeta function",
    "text": "sim_ordbeta function\nFinally, we can also simulate data from the ordered beta regression model with the sim_ordbeta function. This is useful either for examining how different parameters interact with each other, or more generally for power calculation by iterating over different possible sample sizes. I demonstrate the function here, though note that the vignette loads saved simulation results unless run_model is set to TRUE. Because each simulation draw has to estimate a model, it can take some time to do calculations. Using multiple cores a la the cores option is strongly encouraged to reduce processing time.\nTo access the data simulated for each run, the return_data=TRUE option can be set. To get a single simulated dataset, simply use this option combined with a single iteration and set of parameter values. The data are saved as a list in the column data in the returned data frame. The chunk below examines the first 10 rows of a single simulated dataset (note that the rows are repeated k times for each iteration, while there is one unique simulated dataset per iteration). Each predictor is listed as a Var column from 1 to k, while the simulated outcome is in the outcome column.\n\n\n# NOT RUN IN THE VIGNETTE\n\nsingle_data &lt;- sim_ordbeta(N=100,iter=1,\n                           return_data=T)\n\n# examine the first dataset\n\nknitr::kable(head(single_data$data[[1]]))\n\nBy default, the function simulates continuous predictors. To simulate binary variables, such as in a standard experimental design, use the beta_type function to specify \"binary\" predictors and treat_assign to determine the proportion assigned to treatment for each predictor. For a standard design with only one treatment variable, we’ll also specify that k=1 for a single covariate. Finally, to estimate a reasonable treatment effect, we will specify that beta_coef=0.5, which equals an increase of .5 on the logit scale. While it can be tricky to know a priori what the marginal effect will be (i.e., the actual change in the outcome), the function will calculate marginal effects and report them, so you can play with other options to see what gets you marginal effects/treatment effects of interest.\n\n\nif(run_model) {\n  \n  sim_data &lt;- sim_ordbeta(N=c(250,500,750),\n                          k=1,\n                          beta_coef = .5,\n                          iter=100,cores=10,\n                          beta_type=\"binary\",\n                          treat_assign=0.3)\n  \n} else {\n  \n  data(\"sim_data\")\n  \n}\n\nFor example, in the simulation above, the returned data frame stores the true marginal effect in the marg_eff column, and lists it as 0.284, which is quite a large effect for a \\([0,1]\\) outcome. The following plot shows some of the summary statistics derived by aggregating over the iterations of the simulation. Some of the notable statistics that are included are power (for all covariates \\(k\\)), S errors (wrong sign of the estimated effect) and M errors (magnitude of bias). As can be seen, issues of bias decline markedly for this treatment effect size and a sample of 500 or greater has more than enough power.\n\n\nsim_data %&gt;% \n    select(`Proportion S Errors`=\"s_err\",N,Power=\"power\",\n         `M Errors`=\"m_err\",Variance=\"var_marg\") %&gt;% \n  gather(key = \"type\",value=\"estimate\",-N) %&gt;%\n  ggplot(aes(y=estimate,x=N)) +\n  #geom_point(aes(colour=model),alpha=0.1) +\n  stat_summary(fun.data=\"mean_cl_boot\") + \n  ylab(\"\") +\n  xlab(\"N\") +\n  scale_x_continuous(breaks=c(250,500,750)) +\n  scale_color_viridis_d() +\n  facet_wrap(~type,scales=\"free_y\",ncol = 2) +\n  labs(caption=stringr::str_wrap(\"Summary statistics calculated as mean with bootstrapped confidence interval from simulation draws. M Errors  and S errors are magnitude of bias (+1 equals no bias) and incorrect sign of the estimated marginal effect respectively. Variance refers to estimated posterior variance (uncertainty) of the marginal effect(s).\",width=50)) +\n  guides(color=guide_legend(title=\"\"),\n         linetype=guide_legend(title=\"\")) +\n  theme_minimal() +\n  theme(plot.caption = element_text(size=7),\n        axis.text.x=element_text(size=8))\n\n\n\n\n\n\n\n\nWhile the sim_ordbeta function has the ability to iterate over N, it is of course possible to do more complex experiments by wrapping the function in a loop and passing different values of other parameters, such as treat_assign.",
    "crumbs": [
      "Articles",
      "Introduction to `ordbetareg`"
    ]
  },
  {
    "objectID": "vignettes/package_introduction.html#more-powerful-power-analyses-with-declaredesign",
    "href": "vignettes/package_introduction.html#more-powerful-power-analyses-with-declaredesign",
    "title": "Introduction to ordbetareg",
    "section": "More Powerful Power Analyses with DeclareDesign",
    "text": "More Powerful Power Analyses with DeclareDesign\nThe ordered beta distribution can also be used in tandem with DeclareDesign, a robust R software package for power analysis (see free online book here). To do so, we use a helper function around the R package marginaleffects to allow for DeclareDesign to calculate the effect of a treatment on a bounded 0-1 outcome. We also make use of the ordbetareg functionality of the glmmTMB package (https://github.com/glmmTMB/glmmTMB) as we need to fit many models very quickly.\nYou could also substitute the true ordbetareg function here, although that would require fitting many models with brms models. Of course, if you are using unique features in brms like missing data imputation or multivariate modeling, then you’ll need to replace the glmmTMB code below with ordbetareg::ordbetareg.\nIn the code below I use the ordered beta distribution function rordbeta to simulate \\(Y\\) given an average response \\(mu\\) and a dispersion parameter value for \\(phi\\). I also have to pass cutpoints on the logit scale.\n\n\nlibrary(DeclareDesign)\n#&gt; Loading required package: randomizr\n#&gt; Loading required package: fabricatr\n#&gt; Loading required package: estimatr\n#&gt; \n#&gt; Attaching package: 'DeclareDesign'\n#&gt; The following object is masked from 'package:modelsummary':\n#&gt; \n#&gt;     get_estimates\n#&gt; The following object is masked from 'package:ggplot2':\n#&gt; \n#&gt;     vars\n#&gt; The following object is masked from 'package:dplyr':\n#&gt; \n#&gt;     vars\n\n# helper function for glmmTMB ordbetareg fit\n\ntidy_avg_slopes &lt;- function(x) {\n  tidy(avg_slopes(x))\n}\n\n# first, a simulated proportion using rordbeta (ordered beta distribution)\n# see https://osf.io/preprints/socarxiv/2sx6y\n# cutpoints = number of observations at the bound (i.e. 0 or 1)\n# phi = dispersion, a value of 1 means relatively \"flat\"\n\ndesign_rordbeta &lt;-\n  declare_model(\n    N = 100,\n    potential_outcomes(Y ~ rordbeta(N, mu = .5 + .05*Z,phi = 1,\n                                    cutpoints=c(-3,3)\n    ))\n  ) +\n  declare_inquiry(ATE = 0.05) +\n  declare_assignment(Z = complete_ra(N, m = 50)) +\n  declare_measurement(Y = reveal_outcomes(Y ~ Z)) +\n  declare_estimator(Y ~ Z, .method = glmmTMB::glmmTMB, inquiry = \"ATE\",\n                    family=glmmTMB::ordbeta,\n                    .summary= tidy_avg_slopes,\n                    term=\"Z\")\n\nNote that the value for \\(mu\\) must be strictly bounded between 0 and 1. You could use the logit link function (R function plogis) to allow for more sophisticated forms of linear models. DeclareDesign works best on the un-transformed (real) scale here, so I use a value for the ATE of 0.05 to ensure it stays within 0 and 1. I also include a separate predictor \\(Z\\) with an effect of +0.05.\nThe declare_estimator function above is how we include ordbetareg as a model to test. As I mentioned, I pass the glmmTMB fitting function to .method as it uses conventional MLE and is much faster. Generally this works fine as power analysis doesn’t need to have all the fancy features. If you need them, though, you could always pass in ordbetareg::ordbetareg to the .method argument. I also pass in the little helper function tidy_avg_slopes above (be sure to also load the marginaleffects package) to pull out treatment effects on the un-transformed (0 to 1) scale.\nWith this code, you have a fully featured experimental design that can be further analyzed with DeclareDesign’s many options (again, see the book link above). To demonstrate usage, I include code below that compares using ordbetareg to model the outcome as a proportion versus dichotomizing the outcome by rounding to 0 or 1. The comparison shows that the latter method is biased relative to the true ATE effect (!!).\nNote that this code may take some time to run as uses simulation techniques to derive the power code. You can use the future package to parallelize over multiple cores to speed things up.\n\n\n# create two designs with identical treatment effects/expected values\n# first uses rordbeta to generate proportion in [0,1]\n# second uses rordbeta to generate proportion in [0,1], then\n# dichotomizes to 0 or 1 by rounding at 0.5\n# compare to see which has greater power given same number of obsevations\n# & check for bias\n\n# equivalent to dichotomizing (if a proportion)\ndesign_dichot &lt;-\n  declare_model(\n    N = 100,\n    potential_outcomes(Y ~ as.numeric(rordbeta(N, mu = .5 + .05*Z,phi = 1,\n                                               cutpoints=c(-3,3))&gt;0.5))\n  ) +\n  declare_inquiry(ATE = 0.05) +\n  declare_assignment(Z = complete_ra(N, m = 50)) +\n  declare_measurement(Y = reveal_outcomes(Y ~ Z)) +\n  declare_estimator(Y ~ Z, .method = lm_robust, inquiry = \"ATE\")\n\n# NB: DeclareDesign is using lm_robust as it's default estimation\n# However, it should be unbiased for the mean/expected value for the\n# binomial/dichotomized model\n\ndiagnosands &lt;-\n  declare_diagnosands(bias = mean(estimate - estimand),\n                      power = mean(p.value &lt;= 0.05))\n\n# compare in terms of bias on the ATE & Power\n\ndiagnose_design(design_rordbeta, diagnosands = diagnosands)\ndiagnose_design(design_dichot, diagnosands = diagnosands)",
    "crumbs": [
      "Articles",
      "Introduction to `ordbetareg`"
    ]
  },
  {
    "objectID": "man/pp_check_ordbeta.html",
    "href": "man/pp_check_ordbeta.html",
    "title": "ordbetareg",
    "section": "",
    "text": "The standard brms::pp_check plot available via brms is not accurate for ordbetareg models because an ordered beta regression has both continuous and discrete components. This function implements a bar plot and a density plot for the continuous and discrete elements separately, and will return accurate posterior predictive plots relative to the data.\n\n\n\npp_check_ordbeta(\n  model = NULL,\n  dv = NULL,\n  type = \"both\",\n  ndraws = 10,\n  cores = NULL,\n  group = NULL,\n  new_theme = NULL,\n  outcome_label = NULL,\n  animate = FALSE,\n  reverse_bounds = TRUE,\n  facet_scales = \"fixed\"\n)\n\n\n\n\n\n\n\nmodel\n\n\nA fitted ordbetareg model.\n\n\n\n\ndv\n\n\nIf you fit a model with multiple DVs/responses, pass the name of the DV as a character value. Note: this must be the same as the name of the column in the data used to fit the model.\n\n\n\n\ntype\n\n\nDefault is \"both\" for creating both a discrete (bar) and continuous (density) plot. Can also be \"discrete\" for only the bar plot for discrete values (0/1) or \"continuous\" for continuous values (density plot).\n\n\n\n\nndraws\n\n\nNumber of posterior draws to use to calculate estimates and show in plot. Defaults to 10.\n\n\n\n\ncores\n\n\nNumber of cores to use to produce posterior predictive distribution. Defaults to NULL or 1 core.\n\n\n\n\ngroup\n\n\nA factor variable of the same number of rows as the data that is used to broduce grouped (faceted) plots of the posterior distribution.\n\n\n\n\nnew_theme\n\n\nAny additional themes to be added to ggplot2 (default is NULL).\n\n\n\n\noutcome_label\n\n\nA character value that will replace the name of the outcome in the plot (default is the name of the response variable in the data frame).\n\n\n\n\nanimate\n\n\nWhether to animate each posterior draw for continuous distributions (defaults to FALSE). Requires installation of the gganimate and transformr R packages.\n\n\n\n\nreverse_bounds\n\n\nWhether to plot data using the original bounds in the data (i.e. not 0 and 1).\n\n\n\n\nfacet_scales\n\n\nThe option passed on to the facet_wrap function in ggplot2 for the type of scale for facetting if passing a variable for group. Defaults to “fixed” scales but can be set to “free_y” to allow probability density/bar count scales to vary or “free” to allow both x and y axes to vary (i.e., also outcome axis ticks).\n\n\n\n\n\n\nIf \"both\", prints both plots and returns a list of both plots as ggplot2 objects. Otherwise, prints and returnst the specific plot as a ggplot2 object.\n\n\n\n\nlibrary(ordbetareg)\n\n\n# need a fitted ordbetareg model\n\ndata(\"ord_fit_mean\")\n\nout_plots &lt;- pp_check_ordbeta(ord_fit_mean)\n\n# view discrete bar plot\n\nout_plots$discrete\n\n\n\n\n\n\n\n# view continuous density plot\n\nout_plots$continuous\n\n\n\n\n\n\n\n# change title using ggplot2 ggtitle function\n\nout_plots$discrete + ggplot2::ggtitle(\"New title\")",
    "crumbs": [
      "Reference",
      "pp_check_ordbeta"
    ]
  },
  {
    "objectID": "man/pp_check_ordbeta.html#accurate-posterior-predictive-plots-for-ordbetareg-models",
    "href": "man/pp_check_ordbeta.html#accurate-posterior-predictive-plots-for-ordbetareg-models",
    "title": "ordbetareg",
    "section": "",
    "text": "The standard brms::pp_check plot available via brms is not accurate for ordbetareg models because an ordered beta regression has both continuous and discrete components. This function implements a bar plot and a density plot for the continuous and discrete elements separately, and will return accurate posterior predictive plots relative to the data.\n\n\n\npp_check_ordbeta(\n  model = NULL,\n  dv = NULL,\n  type = \"both\",\n  ndraws = 10,\n  cores = NULL,\n  group = NULL,\n  new_theme = NULL,\n  outcome_label = NULL,\n  animate = FALSE,\n  reverse_bounds = TRUE,\n  facet_scales = \"fixed\"\n)\n\n\n\n\n\n\n\nmodel\n\n\nA fitted ordbetareg model.\n\n\n\n\ndv\n\n\nIf you fit a model with multiple DVs/responses, pass the name of the DV as a character value. Note: this must be the same as the name of the column in the data used to fit the model.\n\n\n\n\ntype\n\n\nDefault is \"both\" for creating both a discrete (bar) and continuous (density) plot. Can also be \"discrete\" for only the bar plot for discrete values (0/1) or \"continuous\" for continuous values (density plot).\n\n\n\n\nndraws\n\n\nNumber of posterior draws to use to calculate estimates and show in plot. Defaults to 10.\n\n\n\n\ncores\n\n\nNumber of cores to use to produce posterior predictive distribution. Defaults to NULL or 1 core.\n\n\n\n\ngroup\n\n\nA factor variable of the same number of rows as the data that is used to broduce grouped (faceted) plots of the posterior distribution.\n\n\n\n\nnew_theme\n\n\nAny additional themes to be added to ggplot2 (default is NULL).\n\n\n\n\noutcome_label\n\n\nA character value that will replace the name of the outcome in the plot (default is the name of the response variable in the data frame).\n\n\n\n\nanimate\n\n\nWhether to animate each posterior draw for continuous distributions (defaults to FALSE). Requires installation of the gganimate and transformr R packages.\n\n\n\n\nreverse_bounds\n\n\nWhether to plot data using the original bounds in the data (i.e. not 0 and 1).\n\n\n\n\nfacet_scales\n\n\nThe option passed on to the facet_wrap function in ggplot2 for the type of scale for facetting if passing a variable for group. Defaults to “fixed” scales but can be set to “free_y” to allow probability density/bar count scales to vary or “free” to allow both x and y axes to vary (i.e., also outcome axis ticks).\n\n\n\n\n\n\nIf \"both\", prints both plots and returns a list of both plots as ggplot2 objects. Otherwise, prints and returnst the specific plot as a ggplot2 object.\n\n\n\n\nlibrary(ordbetareg)\n\n\n# need a fitted ordbetareg model\n\ndata(\"ord_fit_mean\")\n\nout_plots &lt;- pp_check_ordbeta(ord_fit_mean)\n\n# view discrete bar plot\n\nout_plots$discrete\n\n\n\n\n\n\n\n# view continuous density plot\n\nout_plots$continuous\n\n\n\n\n\n\n\n# change title using ggplot2 ggtitle function\n\nout_plots$discrete + ggplot2::ggtitle(\"New title\")",
    "crumbs": [
      "Reference",
      "pp_check_ordbeta"
    ]
  },
  {
    "objectID": "man/sim_data.html",
    "href": "man/sim_data.html",
    "title": "ordbetareg",
    "section": "",
    "text": "The simulated draws used in the vignette for calculating statistical power.\n\n\n\nsim_data\n\n\n\n\nA dataframe",
    "crumbs": [
      "Reference",
      "sim_data"
    ]
  },
  {
    "objectID": "man/sim_data.html#simulated-ordered-beta-regression-values",
    "href": "man/sim_data.html#simulated-ordered-beta-regression-values",
    "title": "ordbetareg",
    "section": "",
    "text": "The simulated draws used in the vignette for calculating statistical power.\n\n\n\nsim_data\n\n\n\n\nA dataframe",
    "crumbs": [
      "Reference",
      "sim_data"
    ]
  },
  {
    "objectID": "man/ordbetareg.html",
    "href": "man/ordbetareg.html",
    "title": "ordbetareg",
    "section": "",
    "text": "This function allows you to estimate an ordered beta regression model via a formula syntax.\n\n\n\nordbetareg(\n  formula = NULL,\n  data = NULL,\n  true_bounds = NULL,\n  phi_reg = \"none\",\n  use_brm_multiple = FALSE,\n  coef_prior_mean = 0,\n  coef_prior_SD = 5,\n  intercept_prior_mean = NULL,\n  intercept_prior_SD = NULL,\n  phi_prior = 0.1,\n  dirichlet_prior = c(1, 1, 1),\n  phi_coef_prior_mean = 0,\n  phi_coef_prior_SD = 5,\n  phi_intercept_prior_mean = NULL,\n  phi_intercept_prior_SD = NULL,\n  extra_prior = NULL,\n  init = \"0\",\n  return_stancode = FALSE,\n  ...\n)\n\n\n\n\n\n\n\nformula\n\n\nEither an R formula in the form response/DV ~ var1 + var2 etc. or formula object as created/called by the brms brms::bf function. *Please avoid using 0 or Intercept in the formula definition.\n\n\n\n\ndata\n\n\nAn R data frame or tibble containing the variables in the formula\n\n\n\n\ntrue_bounds\n\n\nIf the true bounds of the outcome/response don’t exist in the data, pass a length 2 numeric vector of the minimum and maximum bounds to properly normalize the outcome/response\n\n\n\n\nphi_reg\n\n\nWhether you are including a linear model predicting the dispersion parameter, phi, and/or for the response. If you are including models for both, pass option ‘both’. If you only have an intercept for the outcome (i.e. a 1 in place of covariates), pass ‘only’. If you only have intercepts for phi (such as a varying intercepts/random effects) model, pass the value \"intercepts\". To set priors on these intercepts, use the extra-prior option with the brms::set_prior function (class=\"sd\"). If no model of any kind for phi, the default, pass ‘none’.\n\n\n\n\nuse_brm_multiple\n\n\n(T/F) Whether the model should use brms::brm_multiple for multiple imputation over multiple dataframes passed as a list to the data argument\n\n\n\n\ncoef_prior_mean\n\n\nThe mean of the Normal distribution prior on the regression coefficients (for predicting the mean of the response). Default is 0.\n\n\n\n\ncoef_prior_SD\n\n\nThe SD of the Normal distribution prior on the regression coefficients (for predicting the mean of the response). Default is 5, which makes the prior weakly informative on the logit scale.\n\n\n\n\nintercept_prior_mean\n\n\nThe mean of the Normal distribution prior for the intercept. By default is NULL, which means the intercept receives the same prior as coef_prior_mean. To zero out the intercept, set this parameter to 0 and coef_prior_SD to a very small number (0.01 or smaller). NOTE: the default intercept in brms is centered (mean-subtracted) by default. To use a traditional intercept, either add 0 + Intercept to the formula or specify center=FALSE in the bf formula function for brms. See brms::brmsformula() for more info.\n\n\n\n\nintercept_prior_SD\n\n\nThe SD of the Normal distribution prior for the intercept. By default is NULL, which means the intercept receives the same prior SD as coef_prior_SD.\n\n\n\n\nphi_prior\n\n\nThe mean parameter of the exponential prior on phi, which determines the dispersion of the beta distribution. The default is .1, which equals a mean of 10 and is thus weakly informative on the interval (0.4, 30). If the response has very low variance (i.e. tightly) clusters around a specific value, then decreasing this prior (and increasing the expected value) may be helpful. Checking the value of phi in the output of the model command will reveal if a value of 0.1 (mean of 10) is too small.\n\n\n\n\ndirichlet_prior\n\n\nA vector of three integers corresponding to the prior parameters for the dirchlet distribution (alpha parameter) governing the location of the cutpoints between the components of the response (continuous vs. degenerate). The default is 1 which puts equal probability on degenerate versus continuous responses. Likely only needs to be changed in a repeated sampling situation to stabilize the cutpoint locations across samples.\n\n\n\n\nphi_coef_prior_mean\n\n\nThe mean of the Normal distribution prior on the regression coefficients for predicting phi, the dispersion parameter. Only useful if a linear model is being fit to phi. Default is 0.\n\n\n\n\nphi_coef_prior_SD\n\n\nThe SD of the Normal distribution prior on the regression coefficients for predicting phi, the dispersion parameter. Only useful if a linear model is being fit to phi. Default is 5, which makes the prior weakly informative on the exponential scale.\n\n\n\n\nphi_intercept_prior_mean\n\n\nThe mean of the Normal distribution prior for the phi (dispersion) regression intercept. By default is NULL, which means the intercept receives the same prior as phi_coef_prior_mean. To zero out the intercept, set this parameter to 0 and phi_coef_prior_SD to a very small number (0.01 or smaller).\n\n\n\n\nphi_intercept_prior_SD\n\n\nThe SD of the Normal distribution prior for the phi (dispersion) regression intercept. By default is NULL, which means the intercept receives the same prior SD as phi_coef_prior_SD.\n\n\n\n\nextra_prior\n\n\nAn additional prior, such as a prior for a specific regression coefficient, added to the outcome regression by passing one of the brms functions brms::set_prior or brms::prior_string with appropriate values.\n\n\n\n\ninit\n\n\nThis parameter is used to determine starting values for the Stan sampler to begin Markov Chain Monte Carlo sampling. It is set by default at 0 because the non-linear nature of beta regression means that it is possible to begin with extreme values depending on the scale of the covariates. Setting this to 0 helps the sampler find starting values. It does, on the other hand, limit the ability to detect convergence issues with Rhat statistics. If that is a concern, such as with an experimental feature of brms, set this to “random” to get more robust starting values (just be sure to scale the covariates so they are not too large in absolute size).\n\n\n\n\nreturn_stancode\n\n\nIf TRUE, will pass back the only the Stan code for the model as a character vector rather than fitting the model.\n\n\n\n\n…\n\n\nAll other arguments passed on to the brm function\n\n\n\n\n\n\nThis function is a wrapper around the brms::brm function, which is a powerful Bayesian regression modeling engine using Stan. To fully explore the options available, including dynamic and hierarchical modeling, please see the documentation for the brm function above. As the ordered beta regression model is currently not available in brms natively, this modeling function allows a brms model to be fit with the ordered beta regression distribution.\nFor more information about the model, see the paper here: https://osf.io/preprints/socarxiv/2sx6y/.\nThis function allows you to set priors on the dispersion parameter, the cutpoints, and the regression coefficients (see below for options). However, to add specific priors on individual covariates, you would need to use the brms::set_prior function by specifying an individual covariate (see function documentation) and passing the result of the function call to the extra_prior argument.\nThis function will also automatically normalize the outcome so that it lies in the [0,1] interval, as required by beta regression. For furthur information, see the documentation for the normalize function.\nPriors can be set on a variety of coefficients in the model, see the description of parameters coef_prior_mean and intercept_prior_mean, in addition to setting a custom prior with the extra_prior option. When setting priors on intercepts, it is important to note that by default, all intercepts in brms are centered (the means are subtracted from the data). As a result, a prior set on the default intercept will have a different interpretation than a traditional intercept (i.e. the value of the outcome when the covariates are all zero). To change this setting, use the brms::bf() function as a wrapper around the formula with the option center=FALSE to set priors on a traditional non-centered intercept.\nNote that while brms also supports adding 0 + Intercept to the formula to address this issue, ordbetareg does not support this syntax. Instead, use center=FALSE as an option to brms::bf().\n\n\n\nA brms object fitted with the ordered beta regression distribution.\n\n\n\n\nlibrary(ordbetareg)\n\n# load survey data that comes with the package\n\nlibrary(dplyr)\ndata(\"pew\")\n\n# prepare data\n\nmodel_data &lt;- select(pew,therm,\n             education=\"F_EDUCCAT2_FINAL\",\n             region=\"F_CREGION_FINAL\",\n             income=\"F_INCOME_FINAL\")\n\n# It takes a while to fit the models. Run the code\n# below if you want to load a saved fitted model from the\n# package, otherwise use the model-fitting code\n\ndata(\"ord_fit_mean\")\n\n  \n  # fit the actual model\n\n  if(.Platform$OS.type!=\"windows\") {\n\n    ord_fit_mean &lt;- ordbetareg(formula=therm ~ education + income +\n      (1|region),\n      data=model_data,\n      cores=2,chains=2)\n\n  }\n\nRunning /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c\nusing C compiler: ‘Apple clang version 16.0.0 (clang-1600.0.26.3)’\nusing SDK: ‘’\nclang -arch arm64 -I\"/Library/Frameworks/R.framework/Resources/include\" -DNDEBUG   -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/Rcpp/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/unsupported\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/BH/include\" -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/src/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppParallel/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/rstan/include\" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DUSE_STANC3 -DSTRICT_R_HEADERS  -DBOOST_PHOENIX_NO_VARIADIC_EXPRESSION  -D_HAS_AUTO_PTR_ETC=0  -include '/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp'  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/opt/R/arm64/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c foo.c -o foo.o\nIn file included from &lt;built-in&gt;:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22:\nIn file included from /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/Eigen/Dense:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/Eigen/Core:19:\n/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:679:10: fatal error: 'cmath' file not found\n  679 | #include &lt;cmath&gt;\n      |          ^~~~~~~\n1 error generated.\nmake: *** [foo.o] Error 1\n\n# access values of the coefficients\n\nsummary(ord_fit_mean)\n\n Family: ord_beta_reg \n  Links: mu = identity; phi = identity; cutzero = identity; cutone = identity \nFormula: therm ~ education + income + (1 | region) \n   Data: data (Number of observations: 2525) \n  Draws: 2 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 2000\n\nMultilevel Hyperparameters:\n~region (Number of levels: 4) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.11      0.09     0.01     0.36 1.01      298      464\n\nRegression Coefficients:\n                                      Estimate Est.Error l-95% CI u-95% CI Rhat\nIntercept                                 0.32      0.16     0.02     0.63 1.01\neducationHighschoolgraduate              -0.19      0.14    -0.47     0.07 1.01\neducationSomecollegenodegree             -0.07      0.14    -0.34     0.21 1.00\neducationAssociate’sdegree               -0.03      0.14    -0.31     0.25 1.01\neducationCollegegraduateDsomepostgrad     0.23      0.14    -0.03     0.51 1.01\neducationPostgraduate                     0.53      0.14     0.25     0.82 1.00\neducationDon’tknowDRefused               -0.21      0.42    -0.98     0.63 1.01\nincome10tounder$20000                    -0.09      0.13    -0.33     0.16 1.00\nincome20tounder$30000                     0.09      0.12    -0.16     0.32 1.00\nincome30tounder$40000                    -0.07      0.12    -0.33     0.16 1.00\nincome40tounder$50000                    -0.14      0.12    -0.38     0.11 1.00\nincome50tounder$75000                    -0.15      0.12    -0.38     0.07 1.00\nincome75tounder$100000                   -0.26      0.11    -0.50    -0.04 1.00\nincome100tounder$150000OR                -0.28      0.12    -0.50    -0.06 1.00\nincome$150000ormore                      -0.26      0.12    -0.48    -0.02 1.00\nincomeVOLDontknowDRefused                -0.48      0.14    -0.76    -0.19 1.00\n                                      Bulk_ESS Tail_ESS\nIntercept                                  367      243\neducationHighschoolgraduate                533      907\neducationSomecollegenodegree               458      470\neducationAssociate’sdegree                 526      435\neducationCollegegraduateDsomepostgrad      479      544\neducationPostgraduate                      462      454\neducationDon’tknowDRefused                 690     1250\nincome10tounder$20000                      631      809\nincome20tounder$30000                      654      917\nincome30tounder$40000                      523     1074\nincome40tounder$50000                      571      991\nincome50tounder$75000                      460      837\nincome75tounder$100000                     495      774\nincome100tounder$150000OR                  568      758\nincome$150000ormore                        545      712\nincomeVOLDontknowDRefused                  579      954\n\nFurther Distributional Parameters:\n        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nphi         2.96      0.08     2.77     3.12 1.01      379      144\ncutzero    -2.72      0.10    -2.92    -2.53 1.00      958     1073\ncutone      1.68      0.02     1.64     1.72 1.00      980     1019\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).",
    "crumbs": [
      "Reference",
      "ordbetareg"
    ]
  },
  {
    "objectID": "man/ordbetareg.html#fit-ordered-beta-regression-model",
    "href": "man/ordbetareg.html#fit-ordered-beta-regression-model",
    "title": "ordbetareg",
    "section": "",
    "text": "This function allows you to estimate an ordered beta regression model via a formula syntax.\n\n\n\nordbetareg(\n  formula = NULL,\n  data = NULL,\n  true_bounds = NULL,\n  phi_reg = \"none\",\n  use_brm_multiple = FALSE,\n  coef_prior_mean = 0,\n  coef_prior_SD = 5,\n  intercept_prior_mean = NULL,\n  intercept_prior_SD = NULL,\n  phi_prior = 0.1,\n  dirichlet_prior = c(1, 1, 1),\n  phi_coef_prior_mean = 0,\n  phi_coef_prior_SD = 5,\n  phi_intercept_prior_mean = NULL,\n  phi_intercept_prior_SD = NULL,\n  extra_prior = NULL,\n  init = \"0\",\n  return_stancode = FALSE,\n  ...\n)\n\n\n\n\n\n\n\nformula\n\n\nEither an R formula in the form response/DV ~ var1 + var2 etc. or formula object as created/called by the brms brms::bf function. *Please avoid using 0 or Intercept in the formula definition.\n\n\n\n\ndata\n\n\nAn R data frame or tibble containing the variables in the formula\n\n\n\n\ntrue_bounds\n\n\nIf the true bounds of the outcome/response don’t exist in the data, pass a length 2 numeric vector of the minimum and maximum bounds to properly normalize the outcome/response\n\n\n\n\nphi_reg\n\n\nWhether you are including a linear model predicting the dispersion parameter, phi, and/or for the response. If you are including models for both, pass option ‘both’. If you only have an intercept for the outcome (i.e. a 1 in place of covariates), pass ‘only’. If you only have intercepts for phi (such as a varying intercepts/random effects) model, pass the value \"intercepts\". To set priors on these intercepts, use the extra-prior option with the brms::set_prior function (class=\"sd\"). If no model of any kind for phi, the default, pass ‘none’.\n\n\n\n\nuse_brm_multiple\n\n\n(T/F) Whether the model should use brms::brm_multiple for multiple imputation over multiple dataframes passed as a list to the data argument\n\n\n\n\ncoef_prior_mean\n\n\nThe mean of the Normal distribution prior on the regression coefficients (for predicting the mean of the response). Default is 0.\n\n\n\n\ncoef_prior_SD\n\n\nThe SD of the Normal distribution prior on the regression coefficients (for predicting the mean of the response). Default is 5, which makes the prior weakly informative on the logit scale.\n\n\n\n\nintercept_prior_mean\n\n\nThe mean of the Normal distribution prior for the intercept. By default is NULL, which means the intercept receives the same prior as coef_prior_mean. To zero out the intercept, set this parameter to 0 and coef_prior_SD to a very small number (0.01 or smaller). NOTE: the default intercept in brms is centered (mean-subtracted) by default. To use a traditional intercept, either add 0 + Intercept to the formula or specify center=FALSE in the bf formula function for brms. See brms::brmsformula() for more info.\n\n\n\n\nintercept_prior_SD\n\n\nThe SD of the Normal distribution prior for the intercept. By default is NULL, which means the intercept receives the same prior SD as coef_prior_SD.\n\n\n\n\nphi_prior\n\n\nThe mean parameter of the exponential prior on phi, which determines the dispersion of the beta distribution. The default is .1, which equals a mean of 10 and is thus weakly informative on the interval (0.4, 30). If the response has very low variance (i.e. tightly) clusters around a specific value, then decreasing this prior (and increasing the expected value) may be helpful. Checking the value of phi in the output of the model command will reveal if a value of 0.1 (mean of 10) is too small.\n\n\n\n\ndirichlet_prior\n\n\nA vector of three integers corresponding to the prior parameters for the dirchlet distribution (alpha parameter) governing the location of the cutpoints between the components of the response (continuous vs. degenerate). The default is 1 which puts equal probability on degenerate versus continuous responses. Likely only needs to be changed in a repeated sampling situation to stabilize the cutpoint locations across samples.\n\n\n\n\nphi_coef_prior_mean\n\n\nThe mean of the Normal distribution prior on the regression coefficients for predicting phi, the dispersion parameter. Only useful if a linear model is being fit to phi. Default is 0.\n\n\n\n\nphi_coef_prior_SD\n\n\nThe SD of the Normal distribution prior on the regression coefficients for predicting phi, the dispersion parameter. Only useful if a linear model is being fit to phi. Default is 5, which makes the prior weakly informative on the exponential scale.\n\n\n\n\nphi_intercept_prior_mean\n\n\nThe mean of the Normal distribution prior for the phi (dispersion) regression intercept. By default is NULL, which means the intercept receives the same prior as phi_coef_prior_mean. To zero out the intercept, set this parameter to 0 and phi_coef_prior_SD to a very small number (0.01 or smaller).\n\n\n\n\nphi_intercept_prior_SD\n\n\nThe SD of the Normal distribution prior for the phi (dispersion) regression intercept. By default is NULL, which means the intercept receives the same prior SD as phi_coef_prior_SD.\n\n\n\n\nextra_prior\n\n\nAn additional prior, such as a prior for a specific regression coefficient, added to the outcome regression by passing one of the brms functions brms::set_prior or brms::prior_string with appropriate values.\n\n\n\n\ninit\n\n\nThis parameter is used to determine starting values for the Stan sampler to begin Markov Chain Monte Carlo sampling. It is set by default at 0 because the non-linear nature of beta regression means that it is possible to begin with extreme values depending on the scale of the covariates. Setting this to 0 helps the sampler find starting values. It does, on the other hand, limit the ability to detect convergence issues with Rhat statistics. If that is a concern, such as with an experimental feature of brms, set this to “random” to get more robust starting values (just be sure to scale the covariates so they are not too large in absolute size).\n\n\n\n\nreturn_stancode\n\n\nIf TRUE, will pass back the only the Stan code for the model as a character vector rather than fitting the model.\n\n\n\n\n…\n\n\nAll other arguments passed on to the brm function\n\n\n\n\n\n\nThis function is a wrapper around the brms::brm function, which is a powerful Bayesian regression modeling engine using Stan. To fully explore the options available, including dynamic and hierarchical modeling, please see the documentation for the brm function above. As the ordered beta regression model is currently not available in brms natively, this modeling function allows a brms model to be fit with the ordered beta regression distribution.\nFor more information about the model, see the paper here: https://osf.io/preprints/socarxiv/2sx6y/.\nThis function allows you to set priors on the dispersion parameter, the cutpoints, and the regression coefficients (see below for options). However, to add specific priors on individual covariates, you would need to use the brms::set_prior function by specifying an individual covariate (see function documentation) and passing the result of the function call to the extra_prior argument.\nThis function will also automatically normalize the outcome so that it lies in the [0,1] interval, as required by beta regression. For furthur information, see the documentation for the normalize function.\nPriors can be set on a variety of coefficients in the model, see the description of parameters coef_prior_mean and intercept_prior_mean, in addition to setting a custom prior with the extra_prior option. When setting priors on intercepts, it is important to note that by default, all intercepts in brms are centered (the means are subtracted from the data). As a result, a prior set on the default intercept will have a different interpretation than a traditional intercept (i.e. the value of the outcome when the covariates are all zero). To change this setting, use the brms::bf() function as a wrapper around the formula with the option center=FALSE to set priors on a traditional non-centered intercept.\nNote that while brms also supports adding 0 + Intercept to the formula to address this issue, ordbetareg does not support this syntax. Instead, use center=FALSE as an option to brms::bf().\n\n\n\nA brms object fitted with the ordered beta regression distribution.\n\n\n\n\nlibrary(ordbetareg)\n\n# load survey data that comes with the package\n\nlibrary(dplyr)\ndata(\"pew\")\n\n# prepare data\n\nmodel_data &lt;- select(pew,therm,\n             education=\"F_EDUCCAT2_FINAL\",\n             region=\"F_CREGION_FINAL\",\n             income=\"F_INCOME_FINAL\")\n\n# It takes a while to fit the models. Run the code\n# below if you want to load a saved fitted model from the\n# package, otherwise use the model-fitting code\n\ndata(\"ord_fit_mean\")\n\n  \n  # fit the actual model\n\n  if(.Platform$OS.type!=\"windows\") {\n\n    ord_fit_mean &lt;- ordbetareg(formula=therm ~ education + income +\n      (1|region),\n      data=model_data,\n      cores=2,chains=2)\n\n  }\n\nRunning /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c\nusing C compiler: ‘Apple clang version 16.0.0 (clang-1600.0.26.3)’\nusing SDK: ‘’\nclang -arch arm64 -I\"/Library/Frameworks/R.framework/Resources/include\" -DNDEBUG   -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/Rcpp/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/unsupported\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/BH/include\" -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/src/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppParallel/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/rstan/include\" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DUSE_STANC3 -DSTRICT_R_HEADERS  -DBOOST_PHOENIX_NO_VARIADIC_EXPRESSION  -D_HAS_AUTO_PTR_ETC=0  -include '/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp'  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/opt/R/arm64/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c foo.c -o foo.o\nIn file included from &lt;built-in&gt;:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22:\nIn file included from /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/Eigen/Dense:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/Eigen/Core:19:\n/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:679:10: fatal error: 'cmath' file not found\n  679 | #include &lt;cmath&gt;\n      |          ^~~~~~~\n1 error generated.\nmake: *** [foo.o] Error 1\n\n# access values of the coefficients\n\nsummary(ord_fit_mean)\n\n Family: ord_beta_reg \n  Links: mu = identity; phi = identity; cutzero = identity; cutone = identity \nFormula: therm ~ education + income + (1 | region) \n   Data: data (Number of observations: 2525) \n  Draws: 2 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 2000\n\nMultilevel Hyperparameters:\n~region (Number of levels: 4) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.11      0.09     0.01     0.36 1.01      298      464\n\nRegression Coefficients:\n                                      Estimate Est.Error l-95% CI u-95% CI Rhat\nIntercept                                 0.32      0.16     0.02     0.63 1.01\neducationHighschoolgraduate              -0.19      0.14    -0.47     0.07 1.01\neducationSomecollegenodegree             -0.07      0.14    -0.34     0.21 1.00\neducationAssociate’sdegree               -0.03      0.14    -0.31     0.25 1.01\neducationCollegegraduateDsomepostgrad     0.23      0.14    -0.03     0.51 1.01\neducationPostgraduate                     0.53      0.14     0.25     0.82 1.00\neducationDon’tknowDRefused               -0.21      0.42    -0.98     0.63 1.01\nincome10tounder$20000                    -0.09      0.13    -0.33     0.16 1.00\nincome20tounder$30000                     0.09      0.12    -0.16     0.32 1.00\nincome30tounder$40000                    -0.07      0.12    -0.33     0.16 1.00\nincome40tounder$50000                    -0.14      0.12    -0.38     0.11 1.00\nincome50tounder$75000                    -0.15      0.12    -0.38     0.07 1.00\nincome75tounder$100000                   -0.26      0.11    -0.50    -0.04 1.00\nincome100tounder$150000OR                -0.28      0.12    -0.50    -0.06 1.00\nincome$150000ormore                      -0.26      0.12    -0.48    -0.02 1.00\nincomeVOLDontknowDRefused                -0.48      0.14    -0.76    -0.19 1.00\n                                      Bulk_ESS Tail_ESS\nIntercept                                  367      243\neducationHighschoolgraduate                533      907\neducationSomecollegenodegree               458      470\neducationAssociate’sdegree                 526      435\neducationCollegegraduateDsomepostgrad      479      544\neducationPostgraduate                      462      454\neducationDon’tknowDRefused                 690     1250\nincome10tounder$20000                      631      809\nincome20tounder$30000                      654      917\nincome30tounder$40000                      523     1074\nincome40tounder$50000                      571      991\nincome50tounder$75000                      460      837\nincome75tounder$100000                     495      774\nincome100tounder$150000OR                  568      758\nincome$150000ormore                        545      712\nincomeVOLDontknowDRefused                  579      954\n\nFurther Distributional Parameters:\n        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nphi         2.96      0.08     2.77     3.12 1.01      379      144\ncutzero    -2.72      0.10    -2.92    -2.53 1.00      958     1073\ncutone      1.68      0.02     1.64     1.72 1.00      980     1019\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).",
    "crumbs": [
      "Reference",
      "ordbetareg"
    ]
  },
  {
    "objectID": "man/fit_multivariate.html",
    "href": "man/fit_multivariate.html",
    "title": "ordbetareg",
    "section": "",
    "text": "A fitted ordered beta regression model with two responses, one an ordered beta regression and the other a Gaussian/Normal outcome. Useful for examining mediation analysis.\n\n\n\nfit_multivariate\n\n\n\n\nan ordbetareg object",
    "crumbs": [
      "Reference",
      "fit_multivariate"
    ]
  },
  {
    "objectID": "man/fit_multivariate.html#fitted-ordered-beta-regression-model-multivariate-regression",
    "href": "man/fit_multivariate.html#fitted-ordered-beta-regression-model-multivariate-regression",
    "title": "ordbetareg",
    "section": "",
    "text": "A fitted ordered beta regression model with two responses, one an ordered beta regression and the other a Gaussian/Normal outcome. Useful for examining mediation analysis.\n\n\n\nfit_multivariate\n\n\n\n\nan ordbetareg object",
    "crumbs": [
      "Reference",
      "fit_multivariate"
    ]
  },
  {
    "objectID": "man/ord_fit_phi.html",
    "href": "man/ord_fit_phi.html",
    "title": "ordbetareg",
    "section": "",
    "text": "A fitted ordered beta regression model to the dispersion parameter of the thermometer column from the pew data.\n\n\n\nord_fit_phi\n\n\n\n\nan ordbetareg object",
    "crumbs": [
      "Reference",
      "ord_fit_phi"
    ]
  },
  {
    "objectID": "man/ord_fit_phi.html#fitted-ordered-beta-regression-model-phi-regression",
    "href": "man/ord_fit_phi.html#fitted-ordered-beta-regression-model-phi-regression",
    "title": "ordbetareg",
    "section": "",
    "text": "A fitted ordered beta regression model to the dispersion parameter of the thermometer column from the pew data.\n\n\n\nord_fit_phi\n\n\n\n\nan ordbetareg object",
    "crumbs": [
      "Reference",
      "ord_fit_phi"
    ]
  },
  {
    "objectID": "man/sim_ordbeta.html",
    "href": "man/sim_ordbeta.html",
    "title": "ordbetareg",
    "section": "",
    "text": "This function allows you to calculate power curves (or anything else) via simulating the ordered beta regression model.\n\n\n\nsim_ordbeta(\n  N = 1000,\n  k = 5,\n  iter = 1000,\n  cores = 1,\n  phi = 1,\n  cutpoints = c(-1, 1),\n  beta_coef = NULL,\n  beta_type = \"continuous\",\n  treat_assign = 0.5,\n  return_data = FALSE,\n  seed = as.numeric(Sys.time()),\n  ...\n)\n\n\n\n\n\n\n\nN\n\n\nThe sample size for the simulation. Include a vector of integers to examine power/results for multiple sample sizes.\n\n\n\n\nk\n\n\nThe number of covariates/predictors.\n\n\n\n\niter\n\n\nThe number of simulations to run. For power calculation, should be at least 500 (yes, this will take some time).\n\n\n\n\ncores\n\n\nThe number of cores to use to parallelize the simulation.\n\n\n\n\nphi\n\n\nValue of the dispersion parameter in the beta distribution.\n\n\n\n\ncutpoints\n\n\nValue of the two cutpoints for the ordered model. By default are the values -1 and +1 (these are interpreted in the logit scale and so should not be too large). The farther apart, the fewer degenerate (0 or 1) responses there will be in the distribution.\n\n\n\n\nbeta_coef\n\n\nIf not null, a vector of length k of the true predictor coefficients/treatment values to use for the simulation. Otherwise, coefficients are drawn from a random uniform distribution from -1 to 1 for each predictor.\n\n\n\n\nbeta_type\n\n\nCan be either continuous or binary. Use the latter for conventional treatments with two values.\n\n\n\n\ntreat_assign\n\n\nIf beta_type is set to binary, you can use this parameter to set the proportion of N assigned to treatment. By default, the parameter is set to 0.5 for equal/balanced treatment control groups.\n\n\n\n\nreturn_data\n\n\nWhether to return the simulated dqta as a list in the data column of the returned data frame.\n\n\n\n\nseed\n\n\nThe seed to use to make the results reproducible. Set automatically to a date-time stamp.\n\n\n\n\n…\n\n\nAny other arguments are passed on to the brms::brm function to control modeling options.\n\n\n\n\n\n\nThis function implements the simulation found in Kubinec (2022). This simulation allows you to vary the sample size, number & type of predictors, values of the predictors (or treatment values), and the power to target. The function returns a data frame with one row per simulation draw and covariate k.\n\n\n\na tibble data frame with columns of simulated and estimated values and rows for each simulation iteration X coefficient combination. I.e., if there are five predictors, and 1,000 iterations, the resulting data frame will have 1,000 rows. If there are multiple values for N, then each value of N will have its own set of iterations, making the final size of the data a multiple of the number of sample sizes to iterate over. The data frame will have the following columns: 1.\n\n\n\n\nlibrary(ordbetareg)\n\n# This function takes a while to run as it has\n# to fit an ordered beta regression to each\n# draw. The package comes with a saved\n# simulation dataset you can inspect to see what the\n# result looks like\n\ndata(\"sim_data\")\n\nlibrary(dplyr)\n\n# will take a while to run this\n\n\n  if(.Platform$OS.type!=\"windows\") {\n\n    sim_data &lt;- sim_ordbeta(N=c(250,750),\n    k=1,\n    beta_coef = .5,\n    iter=5,cores=2,\n    beta_type=\"binary\",\n    treat_assign=0.3)\n\n    }\n\nRunning /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c\nusing C compiler: ‘Apple clang version 16.0.0 (clang-1600.0.26.3)’\nusing SDK: ‘’\nclang -arch arm64 -I\"/Library/Frameworks/R.framework/Resources/include\" -DNDEBUG   -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/Rcpp/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/unsupported\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/BH/include\" -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/src/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppParallel/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/rstan/include\" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DUSE_STANC3 -DSTRICT_R_HEADERS  -DBOOST_PHOENIX_NO_VARIADIC_EXPRESSION  -D_HAS_AUTO_PTR_ETC=0  -include '/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp'  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/opt/R/arm64/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c foo.c -o foo.o\nIn file included from &lt;built-in&gt;:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22:\nIn file included from /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/Eigen/Dense:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/Eigen/Core:19:\n/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:679:10: fatal error: 'cmath' file not found\n  679 | #include &lt;cmath&gt;\n      |          ^~~~~~~\n1 error generated.\nmake: *** [foo.o] Error 1\n\n# to get the power values by N, simply summarize/group\n# by N with functions from the R package dplyr\n\nsim_data %&gt;%\n  group_by(N) %&gt;%\n  summarize(mean_power=mean(power))\n\n# A tibble: 2 × 2\n      N mean_power\n  &lt;dbl&gt;      &lt;dbl&gt;\n1   250          1\n2   750          1",
    "crumbs": [
      "Reference",
      "sim_ordbeta"
    ]
  },
  {
    "objectID": "man/sim_ordbeta.html#power-calculation-via-simulation-of-the-ordered-beta-regression-model",
    "href": "man/sim_ordbeta.html#power-calculation-via-simulation-of-the-ordered-beta-regression-model",
    "title": "ordbetareg",
    "section": "",
    "text": "This function allows you to calculate power curves (or anything else) via simulating the ordered beta regression model.\n\n\n\nsim_ordbeta(\n  N = 1000,\n  k = 5,\n  iter = 1000,\n  cores = 1,\n  phi = 1,\n  cutpoints = c(-1, 1),\n  beta_coef = NULL,\n  beta_type = \"continuous\",\n  treat_assign = 0.5,\n  return_data = FALSE,\n  seed = as.numeric(Sys.time()),\n  ...\n)\n\n\n\n\n\n\n\nN\n\n\nThe sample size for the simulation. Include a vector of integers to examine power/results for multiple sample sizes.\n\n\n\n\nk\n\n\nThe number of covariates/predictors.\n\n\n\n\niter\n\n\nThe number of simulations to run. For power calculation, should be at least 500 (yes, this will take some time).\n\n\n\n\ncores\n\n\nThe number of cores to use to parallelize the simulation.\n\n\n\n\nphi\n\n\nValue of the dispersion parameter in the beta distribution.\n\n\n\n\ncutpoints\n\n\nValue of the two cutpoints for the ordered model. By default are the values -1 and +1 (these are interpreted in the logit scale and so should not be too large). The farther apart, the fewer degenerate (0 or 1) responses there will be in the distribution.\n\n\n\n\nbeta_coef\n\n\nIf not null, a vector of length k of the true predictor coefficients/treatment values to use for the simulation. Otherwise, coefficients are drawn from a random uniform distribution from -1 to 1 for each predictor.\n\n\n\n\nbeta_type\n\n\nCan be either continuous or binary. Use the latter for conventional treatments with two values.\n\n\n\n\ntreat_assign\n\n\nIf beta_type is set to binary, you can use this parameter to set the proportion of N assigned to treatment. By default, the parameter is set to 0.5 for equal/balanced treatment control groups.\n\n\n\n\nreturn_data\n\n\nWhether to return the simulated dqta as a list in the data column of the returned data frame.\n\n\n\n\nseed\n\n\nThe seed to use to make the results reproducible. Set automatically to a date-time stamp.\n\n\n\n\n…\n\n\nAny other arguments are passed on to the brms::brm function to control modeling options.\n\n\n\n\n\n\nThis function implements the simulation found in Kubinec (2022). This simulation allows you to vary the sample size, number & type of predictors, values of the predictors (or treatment values), and the power to target. The function returns a data frame with one row per simulation draw and covariate k.\n\n\n\na tibble data frame with columns of simulated and estimated values and rows for each simulation iteration X coefficient combination. I.e., if there are five predictors, and 1,000 iterations, the resulting data frame will have 1,000 rows. If there are multiple values for N, then each value of N will have its own set of iterations, making the final size of the data a multiple of the number of sample sizes to iterate over. The data frame will have the following columns: 1.\n\n\n\n\nlibrary(ordbetareg)\n\n# This function takes a while to run as it has\n# to fit an ordered beta regression to each\n# draw. The package comes with a saved\n# simulation dataset you can inspect to see what the\n# result looks like\n\ndata(\"sim_data\")\n\nlibrary(dplyr)\n\n# will take a while to run this\n\n\n  if(.Platform$OS.type!=\"windows\") {\n\n    sim_data &lt;- sim_ordbeta(N=c(250,750),\n    k=1,\n    beta_coef = .5,\n    iter=5,cores=2,\n    beta_type=\"binary\",\n    treat_assign=0.3)\n\n    }\n\nRunning /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c\nusing C compiler: ‘Apple clang version 16.0.0 (clang-1600.0.26.3)’\nusing SDK: ‘’\nclang -arch arm64 -I\"/Library/Frameworks/R.framework/Resources/include\" -DNDEBUG   -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/Rcpp/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/unsupported\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/BH/include\" -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/src/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppParallel/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/rstan/include\" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DUSE_STANC3 -DSTRICT_R_HEADERS  -DBOOST_PHOENIX_NO_VARIADIC_EXPRESSION  -D_HAS_AUTO_PTR_ETC=0  -include '/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp'  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/opt/R/arm64/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c foo.c -o foo.o\nIn file included from &lt;built-in&gt;:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22:\nIn file included from /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/Eigen/Dense:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/Eigen/Core:19:\n/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:679:10: fatal error: 'cmath' file not found\n  679 | #include &lt;cmath&gt;\n      |          ^~~~~~~\n1 error generated.\nmake: *** [foo.o] Error 1\n\n# to get the power values by N, simply summarize/group\n# by N with functions from the R package dplyr\n\nsim_data %&gt;%\n  group_by(N) %&gt;%\n  summarize(mean_power=mean(power))\n\n# A tibble: 2 × 2\n      N mean_power\n  &lt;dbl&gt;      &lt;dbl&gt;\n1   250          1\n2   750          1",
    "crumbs": [
      "Reference",
      "sim_ordbeta"
    ]
  },
  {
    "objectID": "man/rordbeta.html",
    "href": "man/rordbeta.html",
    "title": "ordbetareg",
    "section": "",
    "text": "This function will generate ordered beta random variates given values for the mean (mu), dispersion (phi) and cutpoints governing the ratio of degenerate (discrete) to continuous responses.\n\n\n\nrordbeta(n = 100, mu = 0.5, phi = 1, cutpoints = c(-1, 1))\n\n\n\n\n\n\n\nn\n\n\nNumber of variates to generate.\n\n\n\n\nmu\n\n\nValue of the mean of the distribution. Should be in the (0,1) interval (cannot be strictly equal to 0 or 1). If length is greater than 1, should be of length n.\n\n\n\n\nphi\n\n\nValue of the dispersion parameter. Should be strictly greater than 0. If length is greater than 1, should be of length n.\n\n\n\n\ncutpoints\n\n\nA vector of two numeric values for the cutpoints. Second value should be strictly greater than the first value.\n\n\n\n\n\n\nA vector of length n of variates from the ordered beta distribution.\n\n\n\n\nlibrary(ordbetareg)\n\n\n# generate 100 random variates with an average of 0.7\n# all will be in the closed interval \\[0,1\\]\n\nordbeta_var &lt;- rordbeta(n=100, mu=0.7, phi=2)\n\n# Will be approx mean = 0.7 with high positive skew\n\nsummary(ordbeta_var)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0000  0.4346  0.9619  0.7193  1.0000  1.0000",
    "crumbs": [
      "Reference",
      "rordbeta"
    ]
  },
  {
    "objectID": "man/rordbeta.html#generate-ordered-beta-variates",
    "href": "man/rordbeta.html#generate-ordered-beta-variates",
    "title": "ordbetareg",
    "section": "",
    "text": "This function will generate ordered beta random variates given values for the mean (mu), dispersion (phi) and cutpoints governing the ratio of degenerate (discrete) to continuous responses.\n\n\n\nrordbeta(n = 100, mu = 0.5, phi = 1, cutpoints = c(-1, 1))\n\n\n\n\n\n\n\nn\n\n\nNumber of variates to generate.\n\n\n\n\nmu\n\n\nValue of the mean of the distribution. Should be in the (0,1) interval (cannot be strictly equal to 0 or 1). If length is greater than 1, should be of length n.\n\n\n\n\nphi\n\n\nValue of the dispersion parameter. Should be strictly greater than 0. If length is greater than 1, should be of length n.\n\n\n\n\ncutpoints\n\n\nA vector of two numeric values for the cutpoints. Second value should be strictly greater than the first value.\n\n\n\n\n\n\nA vector of length n of variates from the ordered beta distribution.\n\n\n\n\nlibrary(ordbetareg)\n\n\n# generate 100 random variates with an average of 0.7\n# all will be in the closed interval \\[0,1\\]\n\nordbeta_var &lt;- rordbeta(n=100, mu=0.7, phi=2)\n\n# Will be approx mean = 0.7 with high positive skew\n\nsummary(ordbeta_var)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0000  0.4346  0.9619  0.7193  1.0000  1.0000",
    "crumbs": [
      "Reference",
      "rordbeta"
    ]
  },
  {
    "objectID": "man/plot_heiss.html",
    "href": "man/plot_heiss.html",
    "title": "ordbetareg",
    "section": "",
    "text": "The Heiss plot, developed by the statistician Andrew Heiss, is a plot of the predicted proportions of components on a bounded scale that are grouped by the unique levels of a grouping variable or factor (such as a random effect) in the model. The plot excels at showing how the scale components–that is, the bottom, middle continuous, and top ends of the scale–vary with a discrete variable while also capturing posterior uncertainty. This plot was the winner of the 2023 ordbetareg Visualization Prize.\n\n\n\nplot_heiss(\n  object,\n  grouping_fac = NULL,\n  recode_group_labels = NULL,\n  ndraws = NULL,\n  show_category_perc_labels = TRUE,\n  category_label_font_size = 3,\n  category_label_accuracy = 1,\n  strip_text_font = element_text(face = \"plain\", size = 9),\n  plot_title = \"Predicted Proportions of Bounded Scale Components\",\n  plot_subtitle = paste0(\"By Unique Values of \", grouping_fac),\n  plot_caption =\n    \"Plot shows predicted proportions of the components of a bounded scale, i.e. the predicted (expected) probability of the top value of the scale, the intermediate continuous values, and the bottom value of the scale. The predictions are subset for unique values of a grouping factor. The predictions are shown for multiple posterior draws to indicate uncertainty. Labels on components indicate posterior quantiles for the probability of that component for each level of the grouping variable.\",\n  plot_caption_width = 70,\n  calc_func = mean,\n  lb = 0.05,\n  upb = 0.95,\n  plot_font_size = 11,\n  plot_font = \"\",\n  y_axis_label = \"Predicted Proportions\",\n  legend_name = \"Scale Components\",\n  component_colors = c(\"#ef8737\", \"#bb292c\", \"#62205f\"),\n  component_labels = c(\"0\", \"(0-1)\", \"1\"),\n  ...\n)\n\n\n\n\n\n\n\nobject\n\n\nA fitted ordbetareg() model object.\n\n\n\n\ngrouping_fac\n\n\nA character string indicating the name of the discrete column in the data used for grouping predictions. Must be a valid column name that was passed to ordbetareg().\n\n\n\n\nrecode_group_labels\n\n\nOptional. A character vector of new labels for the grouping factor levels. Must match the number and order of unique levels/values in grouping_fac.\n\n\n\n\nndraws\n\n\nOptional. The number of posterior draws to use for predictions. If NULL, all available draws are used.\n\n\n\n\nshow_category_perc_labels\n\n\nLogical. Whether to display category percentage labels on the plot. Defaults to TRUE.\n\n\n\n\ncategory_label_font_size\n\n\nThe ggplot2 font size for the labels on the scale components (if show_category_perc_labels is TRUE). Defaults to 3.\n\n\n\n\ncategory_label_accuracy\n\n\nThe accuracy, or amount of rounding, for component label ranges on the plot (if show_category_perc_labels is TRUE). Default is 1. See scales::label_percent() for more info on meaning of accuracy parameter.\n\n\n\n\nstrip_text_font\n\n\nA ggplot2::element_text object defining the font style for facet strip text. Defaults to element_text(face = “plain”, size = 9).\n\n\n\n\nplot_title\n\n\nTitle of the plot. Defaults to \"Predicted Proportions of Bounded Scale Components\".\n\n\n\n\nplot_subtitle\n\n\nSubtitle of the plot. Defaults to a message indicating the grouping variable.\n\n\n\n\nplot_caption\n\n\nCaption text for the plot. Defaults to a detailed description of the plot contents.\n\n\n\n\nplot_caption_width\n\n\nWidth (in characters) at which the caption is wrapped. Defaults to 60.\n\n\n\n\ncalc_func\n\n\nA function used to calculate the central tendency of predictions. Defaults to mean.\n\n\n\n\nlb\n\n\nLower bound for uncertainty intervals. Defaults to 0.05 (5th percentile).\n\n\n\n\nupb\n\n\nUpper bound for uncertainty intervals. Defaults to 0.95 (95th percentile).\n\n\n\n\nplot_font_size\n\n\nBase font size for the plot. Defaults to 11.\n\n\n\n\nplot_font\n\n\nBase font family for the plot. Defaults to an empty string (uses system default).\n\n\n\n\ny_axis_label\n\n\nLabel for the y-axis. Defaults to \"Predicted Proportions\".\n\n\n\n\nlegend_name\n\n\nLegend title. Defaults to \"Scale Components\".\n\n\n\n\ncomponent_colors\n\n\nA character vector of colors for the plot components (bottom, continuous, top). Defaults to c(“#ef8737”, “#bb292c”, “#62205f”).\n\n\n\n\ncomponent_labels\n\n\nA character vector of labels for the scale/outcome components (bottom, continuous, top). Defaults to c(“0”, “(0-1)”, “1”).\n\n\n\n\n…\n\n\nAdditional arguments passed to posterior_epred_ordbeta()).\n\n\n\n\n\n\nFor more details of the plot, see:\nHeiss, Andrew and Ye, Meng. \"Enforcing Boundaries: China’s Overseas NGO Law and Operational Constraints for Global Civil Society.\" Working Paper, 2023. https://stats.andrewheiss.com/compassionate-clam/notebook/manuscript.html.\n\n\n\nA ggplot2 object representing the predicted proportions of the components.\n\n\n\n\nlibrary(ordbetareg)\n\n# Load a fitted model object and create a plot for\n# distinct values of the factor education\n#\n# data('ord_fit_mean')\n#\n# plot_heiss(ord_fit_mean,ndraws=100)\n#\n# See introductory package vignette for more information on function options",
    "crumbs": [
      "Reference",
      "plot_heiss"
    ]
  },
  {
    "objectID": "man/plot_heiss.html#heiss-plot-for-predicted-proportions-of-bounded-scale-components",
    "href": "man/plot_heiss.html#heiss-plot-for-predicted-proportions-of-bounded-scale-components",
    "title": "ordbetareg",
    "section": "",
    "text": "The Heiss plot, developed by the statistician Andrew Heiss, is a plot of the predicted proportions of components on a bounded scale that are grouped by the unique levels of a grouping variable or factor (such as a random effect) in the model. The plot excels at showing how the scale components–that is, the bottom, middle continuous, and top ends of the scale–vary with a discrete variable while also capturing posterior uncertainty. This plot was the winner of the 2023 ordbetareg Visualization Prize.\n\n\n\nplot_heiss(\n  object,\n  grouping_fac = NULL,\n  recode_group_labels = NULL,\n  ndraws = NULL,\n  show_category_perc_labels = TRUE,\n  category_label_font_size = 3,\n  category_label_accuracy = 1,\n  strip_text_font = element_text(face = \"plain\", size = 9),\n  plot_title = \"Predicted Proportions of Bounded Scale Components\",\n  plot_subtitle = paste0(\"By Unique Values of \", grouping_fac),\n  plot_caption =\n    \"Plot shows predicted proportions of the components of a bounded scale, i.e. the predicted (expected) probability of the top value of the scale, the intermediate continuous values, and the bottom value of the scale. The predictions are subset for unique values of a grouping factor. The predictions are shown for multiple posterior draws to indicate uncertainty. Labels on components indicate posterior quantiles for the probability of that component for each level of the grouping variable.\",\n  plot_caption_width = 70,\n  calc_func = mean,\n  lb = 0.05,\n  upb = 0.95,\n  plot_font_size = 11,\n  plot_font = \"\",\n  y_axis_label = \"Predicted Proportions\",\n  legend_name = \"Scale Components\",\n  component_colors = c(\"#ef8737\", \"#bb292c\", \"#62205f\"),\n  component_labels = c(\"0\", \"(0-1)\", \"1\"),\n  ...\n)\n\n\n\n\n\n\n\nobject\n\n\nA fitted ordbetareg() model object.\n\n\n\n\ngrouping_fac\n\n\nA character string indicating the name of the discrete column in the data used for grouping predictions. Must be a valid column name that was passed to ordbetareg().\n\n\n\n\nrecode_group_labels\n\n\nOptional. A character vector of new labels for the grouping factor levels. Must match the number and order of unique levels/values in grouping_fac.\n\n\n\n\nndraws\n\n\nOptional. The number of posterior draws to use for predictions. If NULL, all available draws are used.\n\n\n\n\nshow_category_perc_labels\n\n\nLogical. Whether to display category percentage labels on the plot. Defaults to TRUE.\n\n\n\n\ncategory_label_font_size\n\n\nThe ggplot2 font size for the labels on the scale components (if show_category_perc_labels is TRUE). Defaults to 3.\n\n\n\n\ncategory_label_accuracy\n\n\nThe accuracy, or amount of rounding, for component label ranges on the plot (if show_category_perc_labels is TRUE). Default is 1. See scales::label_percent() for more info on meaning of accuracy parameter.\n\n\n\n\nstrip_text_font\n\n\nA ggplot2::element_text object defining the font style for facet strip text. Defaults to element_text(face = “plain”, size = 9).\n\n\n\n\nplot_title\n\n\nTitle of the plot. Defaults to \"Predicted Proportions of Bounded Scale Components\".\n\n\n\n\nplot_subtitle\n\n\nSubtitle of the plot. Defaults to a message indicating the grouping variable.\n\n\n\n\nplot_caption\n\n\nCaption text for the plot. Defaults to a detailed description of the plot contents.\n\n\n\n\nplot_caption_width\n\n\nWidth (in characters) at which the caption is wrapped. Defaults to 60.\n\n\n\n\ncalc_func\n\n\nA function used to calculate the central tendency of predictions. Defaults to mean.\n\n\n\n\nlb\n\n\nLower bound for uncertainty intervals. Defaults to 0.05 (5th percentile).\n\n\n\n\nupb\n\n\nUpper bound for uncertainty intervals. Defaults to 0.95 (95th percentile).\n\n\n\n\nplot_font_size\n\n\nBase font size for the plot. Defaults to 11.\n\n\n\n\nplot_font\n\n\nBase font family for the plot. Defaults to an empty string (uses system default).\n\n\n\n\ny_axis_label\n\n\nLabel for the y-axis. Defaults to \"Predicted Proportions\".\n\n\n\n\nlegend_name\n\n\nLegend title. Defaults to \"Scale Components\".\n\n\n\n\ncomponent_colors\n\n\nA character vector of colors for the plot components (bottom, continuous, top). Defaults to c(“#ef8737”, “#bb292c”, “#62205f”).\n\n\n\n\ncomponent_labels\n\n\nA character vector of labels for the scale/outcome components (bottom, continuous, top). Defaults to c(“0”, “(0-1)”, “1”).\n\n\n\n\n…\n\n\nAdditional arguments passed to posterior_epred_ordbeta()).\n\n\n\n\n\n\nFor more details of the plot, see:\nHeiss, Andrew and Ye, Meng. \"Enforcing Boundaries: China’s Overseas NGO Law and Operational Constraints for Global Civil Society.\" Working Paper, 2023. https://stats.andrewheiss.com/compassionate-clam/notebook/manuscript.html.\n\n\n\nA ggplot2 object representing the predicted proportions of the components.\n\n\n\n\nlibrary(ordbetareg)\n\n# Load a fitted model object and create a plot for\n# distinct values of the factor education\n#\n# data('ord_fit_mean')\n#\n# plot_heiss(ord_fit_mean,ndraws=100)\n#\n# See introductory package vignette for more information on function options",
    "crumbs": [
      "Reference",
      "plot_heiss"
    ]
  },
  {
    "objectID": "man/posterior_epred_ordbeta.brmsfit.html",
    "href": "man/posterior_epred_ordbeta.brmsfit.html",
    "title": "ordbetareg",
    "section": "",
    "text": "This function is an alternative to the brms default posterior_epred to allow for predictions of the probability of the bottom, top, or middle (i.e. continuous) parts of the response. Useful when wanting to understand what the effect of a covariate is on bottom or top values of the scale.\n\n\n\n## S3 method for class 'brmsfit'\nposterior_epred_ordbeta(\n  object,\n  component = \"all\",\n  newdata = NULL,\n  re_formula = NULL,\n  re.form = NULL,\n  resp = NULL,\n  dpar = NULL,\n  nlpar = NULL,\n  ndraws = NULL,\n  draw_ids = NULL,\n  sort = FALSE,\n  ...\n)\n\n\n\n\n\n\n\nobject\n\n\nAn ordbetareg/brms object\n\n\n\n\ncomponent\n\n\nThe type of response component, i.e., the probability of the bottom end of the scale, the top end, or the middle (i.e.) continuous values.\n\n\n\n\nnewdata\n\n\nsee brms::posterior_epred\n\n\n\n\nre_formula\n\n\nsee brms::posterior_epred\n\n\n\n\nre.form\n\n\nsee brms::posterior_epred\n\n\n\n\nresp\n\n\nsee brms::posterior_epred\n\n\n\n\ndpar\n\n\nsee brms::posterior_epred\n\n\n\n\nnlpar\n\n\nsee brms::posterior_epred\n\n\n\n\nndraws\n\n\nsee brms::posterior_epred\n\n\n\n\ndraw_ids\n\n\nsee brms::posterior_epred\n\n\n\n\nsort\n\n\nsee brms::posterior_epred\n\n\n\n\n…\n\n\nsee brms::posterior_epred\n\n\n\n\n\n\nTo predict the top, bottom, or \"middle\" (i.e. continuous) components of the response, set the component argument to \"top\", \"bottom\" or \"continuous\". By default, component is set to \"all\", which will replicate behavior of the default posterior_epred function.\nAll other arguments besides component are the same as the standard generic posterior_predict. For more information on the relevant arguments for posterior_epred, see brms::posterior_epred.\n\n\n\nAn S x N matrix where S is the number of posterior draws and N is the number of observations.\n\n\n\n\nlibrary(ordbetareg)\n\n\ndata('ord_fit_mean')\n\n# use function to calculate probability of top end of scale\n\npr_1s &lt;- posterior_epred_ordbeta(ord_fit_mean,component=\"top\")\n\n# use function to calculate probability of bottom end of scale\n\npr_0s &lt;- posterior_epred_ordbeta(ord_fit_mean,component=\"top\")\n\n# use function to calculate probability of continuous /\n# beta-distributed part of scale\n\npr_beta &lt;- posterior_epred_ordbeta(ord_fit_mean,component=\"top\")",
    "crumbs": [
      "Reference",
      "posterior_epred_ordbeta.brmsfit"
    ]
  },
  {
    "objectID": "man/posterior_epred_ordbeta.brmsfit.html#calculate-probability-of-response-components",
    "href": "man/posterior_epred_ordbeta.brmsfit.html#calculate-probability-of-response-components",
    "title": "ordbetareg",
    "section": "",
    "text": "This function is an alternative to the brms default posterior_epred to allow for predictions of the probability of the bottom, top, or middle (i.e. continuous) parts of the response. Useful when wanting to understand what the effect of a covariate is on bottom or top values of the scale.\n\n\n\n## S3 method for class 'brmsfit'\nposterior_epred_ordbeta(\n  object,\n  component = \"all\",\n  newdata = NULL,\n  re_formula = NULL,\n  re.form = NULL,\n  resp = NULL,\n  dpar = NULL,\n  nlpar = NULL,\n  ndraws = NULL,\n  draw_ids = NULL,\n  sort = FALSE,\n  ...\n)\n\n\n\n\n\n\n\nobject\n\n\nAn ordbetareg/brms object\n\n\n\n\ncomponent\n\n\nThe type of response component, i.e., the probability of the bottom end of the scale, the top end, or the middle (i.e.) continuous values.\n\n\n\n\nnewdata\n\n\nsee brms::posterior_epred\n\n\n\n\nre_formula\n\n\nsee brms::posterior_epred\n\n\n\n\nre.form\n\n\nsee brms::posterior_epred\n\n\n\n\nresp\n\n\nsee brms::posterior_epred\n\n\n\n\ndpar\n\n\nsee brms::posterior_epred\n\n\n\n\nnlpar\n\n\nsee brms::posterior_epred\n\n\n\n\nndraws\n\n\nsee brms::posterior_epred\n\n\n\n\ndraw_ids\n\n\nsee brms::posterior_epred\n\n\n\n\nsort\n\n\nsee brms::posterior_epred\n\n\n\n\n…\n\n\nsee brms::posterior_epred\n\n\n\n\n\n\nTo predict the top, bottom, or \"middle\" (i.e. continuous) components of the response, set the component argument to \"top\", \"bottom\" or \"continuous\". By default, component is set to \"all\", which will replicate behavior of the default posterior_epred function.\nAll other arguments besides component are the same as the standard generic posterior_predict. For more information on the relevant arguments for posterior_epred, see brms::posterior_epred.\n\n\n\nAn S x N matrix where S is the number of posterior draws and N is the number of observations.\n\n\n\n\nlibrary(ordbetareg)\n\n\ndata('ord_fit_mean')\n\n# use function to calculate probability of top end of scale\n\npr_1s &lt;- posterior_epred_ordbeta(ord_fit_mean,component=\"top\")\n\n# use function to calculate probability of bottom end of scale\n\npr_0s &lt;- posterior_epred_ordbeta(ord_fit_mean,component=\"top\")\n\n# use function to calculate probability of continuous /\n# beta-distributed part of scale\n\npr_beta &lt;- posterior_epred_ordbeta(ord_fit_mean,component=\"top\")",
    "crumbs": [
      "Reference",
      "posterior_epred_ordbeta.brmsfit"
    ]
  },
  {
    "objectID": "man/dordbeta.html",
    "href": "man/dordbeta.html",
    "title": "ordbetareg",
    "section": "",
    "text": "This function will return the density of given variates of the ordered beta distribution conditional on values for the mean (mu), dispersion (phi) and cutpoints governing the ratio of degenerate (discrete) to continuous responses.\n\n\n\ndordbeta(x = 0.9, mu = 0.5, phi = 1, cutpoints = c(-1, 1), log = FALSE)\n\n\n\n\n\n\n\nx\n\n\nVariates of the ordered beta distribution (should be in the [0,1] interval).\n\n\n\n\nmu\n\n\nValue of the mean of the distribution. Should be in the (0,1) interval (cannot be strictly equal to 0 or 1). If length is greater than 1, should be of length x.\n\n\n\n\nphi\n\n\nValue of the dispersion parameter. Should be strictly greater than 0. If length is greater than 1, should be of length x.\n\n\n\n\ncutpoints\n\n\nA vector of two numeric values for the cutpoints. Second value should\n\n\n\n\nlog\n\n\nwhere to return the log density be strictly greater than the first value.\n\n\n\n\n\n\nReturns a vector of length x of the density of the ordered beta distribution conditional on mu and phi.\n\n\n\n\nlibrary(ordbetareg)\n\n\n# examine density (likelihood) of different possible values\n# given fixed values for ordered beta parameters\n\nx &lt;- seq(0, 1, by=0.01)\n\nx_dens &lt;- dordbeta(x, mu = 0.3, phi=2, cutpoints=c(-2, 2))\n\n# Most likely value for x is approx 1\n# Note discontinuity in density function between continuous/discrete values\n# density function is a combined PMF/PDF, so not a real PDF\n# can though be used for MLE\n\nplot(x_dens, x)\n\n\n\n\n\n\n\n# discrete values should be compared to each other:\n# prob of discrete 0 &gt; prob of discrete 1\n\nx_dens[x==0] &gt; x_dens[x==1]\n\n[1] TRUE",
    "crumbs": [
      "Reference",
      "dordbeta"
    ]
  },
  {
    "objectID": "man/dordbeta.html#probability-density-function-for-the-ordered-beta-distribution",
    "href": "man/dordbeta.html#probability-density-function-for-the-ordered-beta-distribution",
    "title": "ordbetareg",
    "section": "",
    "text": "This function will return the density of given variates of the ordered beta distribution conditional on values for the mean (mu), dispersion (phi) and cutpoints governing the ratio of degenerate (discrete) to continuous responses.\n\n\n\ndordbeta(x = 0.9, mu = 0.5, phi = 1, cutpoints = c(-1, 1), log = FALSE)\n\n\n\n\n\n\n\nx\n\n\nVariates of the ordered beta distribution (should be in the [0,1] interval).\n\n\n\n\nmu\n\n\nValue of the mean of the distribution. Should be in the (0,1) interval (cannot be strictly equal to 0 or 1). If length is greater than 1, should be of length x.\n\n\n\n\nphi\n\n\nValue of the dispersion parameter. Should be strictly greater than 0. If length is greater than 1, should be of length x.\n\n\n\n\ncutpoints\n\n\nA vector of two numeric values for the cutpoints. Second value should\n\n\n\n\nlog\n\n\nwhere to return the log density be strictly greater than the first value.\n\n\n\n\n\n\nReturns a vector of length x of the density of the ordered beta distribution conditional on mu and phi.\n\n\n\n\nlibrary(ordbetareg)\n\n\n# examine density (likelihood) of different possible values\n# given fixed values for ordered beta parameters\n\nx &lt;- seq(0, 1, by=0.01)\n\nx_dens &lt;- dordbeta(x, mu = 0.3, phi=2, cutpoints=c(-2, 2))\n\n# Most likely value for x is approx 1\n# Note discontinuity in density function between continuous/discrete values\n# density function is a combined PMF/PDF, so not a real PDF\n# can though be used for MLE\n\nplot(x_dens, x)\n\n\n\n\n\n\n\n# discrete values should be compared to each other:\n# prob of discrete 0 &gt; prob of discrete 1\n\nx_dens[x==0] &gt; x_dens[x==1]\n\n[1] TRUE",
    "crumbs": [
      "Reference",
      "dordbeta"
    ]
  },
  {
    "objectID": "man/pew.html",
    "href": "man/pew.html",
    "title": "ordbetareg",
    "section": "",
    "text": "A dataset with the non-missing responses for the 28th wave of the Pew American Trends Panel survey.\n\n\n\npew\n\n\n\n\nA data frame with 140 variables and 2,538 observations.\n\n\n\nhttps://www.pewresearch.org/social-trends/dataset/american-trends-panel-wave-28/]",
    "crumbs": [
      "Reference",
      "pew"
    ]
  },
  {
    "objectID": "man/pew.html#pew-american-trends-panel-wave-28",
    "href": "man/pew.html#pew-american-trends-panel-wave-28",
    "title": "ordbetareg",
    "section": "",
    "text": "A dataset with the non-missing responses for the 28th wave of the Pew American Trends Panel survey.\n\n\n\npew\n\n\n\n\nA data frame with 140 variables and 2,538 observations.\n\n\n\nhttps://www.pewresearch.org/social-trends/dataset/american-trends-panel-wave-28/]",
    "crumbs": [
      "Reference",
      "pew"
    ]
  },
  {
    "objectID": "man/ord_fit_mean.html",
    "href": "man/ord_fit_mean.html",
    "title": "ordbetareg",
    "section": "",
    "text": "A fitted ordered beta regression model to the mean of the thermometer column from the pew data.\n\n\n\nord_fit_mean\n\n\n\n\nan ordbetareg object",
    "crumbs": [
      "Reference",
      "ord_fit_mean"
    ]
  },
  {
    "objectID": "man/ord_fit_mean.html#fitted-ordered-beta-regression-model",
    "href": "man/ord_fit_mean.html#fitted-ordered-beta-regression-model",
    "title": "ordbetareg",
    "section": "",
    "text": "A fitted ordered beta regression model to the mean of the thermometer column from the pew data.\n\n\n\nord_fit_mean\n\n\n\n\nan ordbetareg object",
    "crumbs": [
      "Reference",
      "ord_fit_mean"
    ]
  },
  {
    "objectID": "man/fit_imputed.html",
    "href": "man/fit_imputed.html",
    "title": "ordbetareg",
    "section": "",
    "text": "A fitted ordered beta regression model on multiple imputed datasets generated by the package mice.\n\n\n\nfit_imputed\n\n\n\n\nan ordbetareg object",
    "crumbs": [
      "Reference",
      "fit_imputed"
    ]
  },
  {
    "objectID": "man/fit_imputed.html#fitted-ordered-beta-regression-model-imputed-datasets",
    "href": "man/fit_imputed.html#fitted-ordered-beta-regression-model-imputed-datasets",
    "title": "ordbetareg",
    "section": "",
    "text": "A fitted ordered beta regression model on multiple imputed datasets generated by the package mice.\n\n\n\nfit_imputed\n\n\n\n\nan ordbetareg object",
    "crumbs": [
      "Reference",
      "fit_imputed"
    ]
  },
  {
    "objectID": "man/normalize.html",
    "href": "man/normalize.html",
    "title": "ordbetareg",
    "section": "",
    "text": "This function takes a continuous (double) column of data and converts it to have 0 as the lower bound and 1 as the upper bound.\n\n\n\nnormalize(outcome, true_bounds = NULL)\n\n\n\n\n\n\n\noutcome\n\n\nAny non-character vector. Factors will be converted to numeric via coercion.\n\n\n\n\ntrue_bounds\n\n\nSpecify this parameter with the lower and upper bound if the observed min/max of the outcome should not be used. Useful when an upper or lower bound exists but the observed data is less than/more than that bound. The normalization function will respect these bounds.\n\n\n\n\n\n\nBeta regression can only be done with a response that is continuous with a lower bound of 0 and an upper bound of 1. However, it is straightforward to transform any lower and upper-bounded continuous variable to the [0,1] interval. This function does the transformation and saves the original bounds as attributes so that the bounds can be reverse-transformed.\n\n\n\nA numeric vector with an upper bound of 1 and a lower bound of 0. The original bounds are saved in the attributes \"lower_bound\" and \"upper_bound\".\n\n\n\n\nlibrary(ordbetareg)\n\n# set up arbitrary upper and lower-bounded vector\noutcome &lt;- runif(1000, min=-33, max=445)\n\n# normalize to \\[0,1\\]\n\ntrans_outcome &lt;- normalize(outcome=outcome)\nsummary(trans_outcome)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0000  0.2452  0.4741  0.4841  0.7302  1.0000 \n\n# only works with numeric vectors and factors\n\n  try(normalize(outcome=c('a','b')))\n\nError in normalize(outcome = c(\"a\", \"b\")) : \n  Please do not pass a character vector as a response/outcome.\nThat really doesn't make any sense.",
    "crumbs": [
      "Reference",
      "normalize"
    ]
  },
  {
    "objectID": "man/normalize.html#normalize-outcomeresponse-to-01-interval",
    "href": "man/normalize.html#normalize-outcomeresponse-to-01-interval",
    "title": "ordbetareg",
    "section": "",
    "text": "This function takes a continuous (double) column of data and converts it to have 0 as the lower bound and 1 as the upper bound.\n\n\n\nnormalize(outcome, true_bounds = NULL)\n\n\n\n\n\n\n\noutcome\n\n\nAny non-character vector. Factors will be converted to numeric via coercion.\n\n\n\n\ntrue_bounds\n\n\nSpecify this parameter with the lower and upper bound if the observed min/max of the outcome should not be used. Useful when an upper or lower bound exists but the observed data is less than/more than that bound. The normalization function will respect these bounds.\n\n\n\n\n\n\nBeta regression can only be done with a response that is continuous with a lower bound of 0 and an upper bound of 1. However, it is straightforward to transform any lower and upper-bounded continuous variable to the [0,1] interval. This function does the transformation and saves the original bounds as attributes so that the bounds can be reverse-transformed.\n\n\n\nA numeric vector with an upper bound of 1 and a lower bound of 0. The original bounds are saved in the attributes \"lower_bound\" and \"upper_bound\".\n\n\n\n\nlibrary(ordbetareg)\n\n# set up arbitrary upper and lower-bounded vector\noutcome &lt;- runif(1000, min=-33, max=445)\n\n# normalize to \\[0,1\\]\n\ntrans_outcome &lt;- normalize(outcome=outcome)\nsummary(trans_outcome)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0000  0.2452  0.4741  0.4841  0.7302  1.0000 \n\n# only works with numeric vectors and factors\n\n  try(normalize(outcome=c('a','b')))\n\nError in normalize(outcome = c(\"a\", \"b\")) : \n  Please do not pass a character vector as a response/outcome.\nThat really doesn't make any sense.",
    "crumbs": [
      "Reference",
      "normalize"
    ]
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "MIT License",
    "section": "",
    "text": "MIT License\nCopyright (c) 2022 ordbetareg authors\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.",
    "crumbs": [
      "License"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ordered Beta Regression",
    "section": "",
    "text": "This website contains information about the R package ordbetareg and general information about other implementations of the ordered beta regression model in other statistical packages. If you just want to get started with the R package ordbetareg, see the vignette.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#about-the-model",
    "href": "index.html#about-the-model",
    "title": "Ordered Beta Regression",
    "section": "About the Model",
    "text": "About the Model\nProportion data–percentiles, slider scales, visual-analog scales–have long puzzled statisticians because they combine two things: a continuous measure (the proportion) and a discrete measure (the top value of 1 or 100% and the bottom value of 0 or 0%). Conventional statistical models like the oft-used OLS regression implicitly assume these boundaries do not exist; this means an OLS regression can predict to absurd values like 115% of patients being sick or -5% of legislators being elected.\nTo address this issue, I developed the ordered beta regression model that combines two things: beta regression, which is defined for any bounded continuous scale, and ordered logit, which works for discrete categories. By doing this, you can fit an ordered beta regression for any percentile/proportion for both the middle continuous part and the bounds of the scale. This model can also predict within this scale as well.\nI describe the model and how to do this type of modeling in this blog post. For more detail on the inner workings of the model, I refer you to the paper:\nKubinec, Robert. 2023. “Ordered Beta Regression: A Parsimonious, Well-Fitting Model for Continuous Data with Lower and Upper Bounds.” Political Analysis 31(4): 519–36. doi: http://doi.org/10.1017/pan.2022.20.\nFor an ungated version of the article, see the pre-publication draft on OSF:\nhttps://osf.io/preprints/socarxiv/2sx6y",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#support-ordbetareg",
    "href": "index.html#support-ordbetareg",
    "title": "Ordered Beta Regression",
    "section": "Support ordbetareg",
    "text": "Support ordbetareg\nIf you’ve enjoyed using the package, consider buying some ordbetareg swag! Click the image below to go to the online store (I receive a few dollars from each order).",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#how-to-use-the-model",
    "href": "index.html#how-to-use-the-model",
    "title": "Ordered Beta Regression",
    "section": "How to Use the Model",
    "text": "How to Use the Model\nAt present, ordered beta regression is available in Stan (Section 3.5), R (Section 3.1), Python (Section 3.2), Stata (Section 3.3), and Julia (Section 3.4). R has the strongest support for the model with multiple implementations, but it is perfectly usable in other statistical frameworks. Below I briefly describe the available packages.\n\nR\nOrdered beta regression is currently available in R in two different packages: ordbetareg and glmmTMB. The primary difference between these two packages is estimation method and the number of ordered beta-specific functions. ordbetareg is a package that estimates a Bayesian implementation as described in the paper above by using brms, a powerful R package for Bayesian regression. ordbetareg is maintained by me and you can see the full source code here on Github (and report an issue with the package if you have one). The package includes auxiliary functions like power analysis and plots that are specific to proportion responses—check out the package vignette for more details.\nglmmTMB, by contrast, is a general purpose regression package that specializes in mixed multilevel models. It implements a broad array of regression models using maximum likelihood. This means that a model estimated in glmmTMB will almost certainly be faster than ordbetareg (although if you set up ordbetareg to use cmdstanr it can get pretty fast). On the other hand, maximum likelihood estimation has some limitations. Arguably the most important one is that it can be tricky to fit a model that doesn’t have observations at the bounds, i.e. either 0 (0%) or 1 (100%). The Bayesian implementation in ordbetareg has no problem doing this.\nThat being said, I think glmmTMB is a fine package and believe it works fine for many scholars’ problems. brms, which ordbetareg is based on, offers a lot more features, including native support for multiple imputation, time series modeling and even latent variable/factor analysis modeling, but it does require more setup and the models are generally slower.\nFor either package, I highly recommend using marginaleffects, and in particular the avg_slopes function, to convert the coefficient estimates in the package to the bounded scale (i.e. between 0 and 1). This function allows you to then interpret your model estimates as the effect of a covariate in terms of percentage change/proportion change in the bounded response/outcome. marginaleffects is available on CRAN and works great with both ordbetareg and glmmTMB.\nWith both of these packages available, ordered beta regression has very strong support in R. If you don’t have a preference for a particular software package and want to use this model, I would recommend R.\n\n\nPython\nOrdered beta regression is implemented using the scipy package with maximum likelihood estimation. At present, you’ll need to clone (i.e., download) this Github repository to install the package as it is not yet available with pip or conda forge. The package includes plotting functions specific to proportion outcomes and reports coefficient values in the untransformed (logit) scale. marginaleffects support for this package is coming soon.\n\n\nStata\nThere is initial support for Stata as a set of .ado files that can be downloaded and installed in the ado folder in your Stata machine. More details are available on the Github repository. This package supports using the margins command to convert coefficients to the bounded outcome scale, although it does not support all Stata features as of yet. There are plans to make this package available via SSC in the near future.\n\n\nJulia\nOrdered beta regression is available via the SubjectiveScalesModels.jl package, which offers support for a variety of regression models useful in cognitive psychology and other fields with sliders/visual analog scales. The package includes neat visualizations and is maintained by Dominique Makowksi.\n\n\nStan\nAs is evident in the paper, the original model was written in Stan code. While I no longer am maintaining the Stan code, it is available in the paper repository and works great. If you want to incorporate the likelihood or other Stan code into your Stan model, feel free!",
    "crumbs": [
      "Home"
    ]
  }
]